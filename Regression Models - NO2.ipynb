{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d388201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ab159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 18:57:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 18:57:26 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/05/07 18:57:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/07 18:57:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM25: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"NO2\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"64g\").config(\"spark.executor.memory\", \"32g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").getOrCreate()\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"../dataset/csv/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2')\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65d85967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 11:47:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|  DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|462000.0|5016000.0|20.0|2019-11-24 00:00:00|         1.6957362|         2.7321966|          50.69704| 4.208417400000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 01:00:00|1.5195319999999999|         2.5212698|         52.217495|3.6308307999999996|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 02:00:00|1.4698639999999998|          2.540143|53.528316000000004|         3.0628326|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 03:00:00|         1.4075043|         2.5400522|         55.269703|          2.444656|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 04:00:00|1.3243808000000001|         2.4868004| 56.09563000000001|         1.9309062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 05:00:00|1.2917066000000001|         2.4511726|56.588367000000005|         1.6992023|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 06:00:00|         1.3729354|         2.5612066|          56.55035|1.5480975000000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 07:00:00|         1.8005681|         2.9130769|          53.39305|         2.1196866|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 08:00:00|         2.7273355|         3.6733246|         45.281013|         4.0332394|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 09:00:00|4.3827419999999995|         5.3303866|         39.011024|         7.3524475|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 10:00:00| 4.251999400000001|         5.3588486|         42.592384|          6.889144|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 11:00:00|3.8547763999999995|          4.963749|42.843109999999996|          6.416884|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 12:00:00|         3.8144705|         4.8981695|         41.092995|6.6946254000000005|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 13:00:00|         3.4577906| 4.584929499999999|42.440692999999996|          6.609909|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 14:00:00|          6.724634|           8.32361|         40.356117|         11.530877|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 15:00:00|         6.9248476| 8.738185000000001|         39.104855|         15.190662|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 16:00:00|         4.1298723|         5.5023584|          37.11807|         13.312062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 17:00:00|          2.274489|            3.0258|         39.018482|         7.7159667|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 18:00:00|         2.2451131|3.0147939999999998|         35.107414|          8.846022|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 19:00:00|         2.5813787|          3.473392|          33.05836|          9.195902|POINT (462000 501...|Lombardia|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lombardia\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"NO2\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/data_chunk/lombardia/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cae4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/19 09:46:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/19 09:46:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|702000.0|4696000.0|20.0|2021-12-11 00:00:00|         1.2706414|1.5203451000000001|60.837047999999996|         2.0367115|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 01:00:00|          1.230251|          1.375211|         57.657654|         2.0870926|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 02:00:00|1.9736630000000002|          2.107126|          54.05386|          2.587756|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 03:00:00|1.7445994999999999|1.8961723000000001|          54.00688|         2.1309106|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 04:00:00|         1.2854077|1.5713663999999998|54.418915000000005|1.6039573999999999|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 05:00:00|0.8523575999999999|         1.1182661| 53.96324499999999|         1.8012519|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 06:00:00|        0.78199506|        0.94059986|         48.937576|          2.225234|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 07:00:00|         1.2314979|         1.4721483|46.245734999999996|         2.2615266|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 08:00:00|         2.3940308|         3.0101209|          51.23545|2.1766273999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 09:00:00|         3.0954514|3.8334242999999995|         51.540134|2.1541189999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 10:00:00|         2.8416717|3.5963568999999995|           54.0782|1.7994093999999998|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 11:00:00|3.1728162999999996|          3.890354|          54.80785|1.9321240000000002|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 12:00:00|         2.9845037|         3.5793805|52.454944999999995|         2.0442376|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 13:00:00|3.3172165999999996|         4.0856614|         52.290283|         2.3790326|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 14:00:00|         3.1035097|         3.9299242|52.074946999999995|2.6943377999999996|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 15:00:00|           2.73482|3.7316300000000004|         52.074814|2.7827827999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 16:00:00|2.6398816000000003|          3.834493|         54.961468|         2.8663425|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 17:00:00|         2.5602984|         3.5236087|          57.12959|          2.730953|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 18:00:00|2.3228847999999997|         3.1005373| 63.62944399999999|         2.3298986|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 19:00:00|         2.3371992|         3.0020676| 64.78435999999999|2.1526132000000002|POINT (702000 469...|  Lazio|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lazio\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"NO2\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/lazio/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db942243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 350:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry| DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|902000.0|4576000.0|20.0|2021-05-21 00:00:00|2.5389232999999995|         2.9607124|         102.89293|         2.4104354|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 01:00:00|         2.5921166|         2.9856164| 98.94353000000001|2.6902790000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 02:00:00|2.5095107999999997|2.8129014999999997|102.21184000000001|2.9086206000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 03:00:00|         2.4757087|         2.7655613|          102.6552|         3.2857485|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 04:00:00|2.4431244999999997|          2.733494|          99.22969|            2.8583|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 05:00:00|          2.602668|         2.9043045|         91.063896|         3.1135564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 06:00:00|         2.9900506|3.3243476999999997|          86.42286|          4.102874|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 07:00:00|         3.1773403|          3.527025| 92.17261500000001|          4.456797|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 08:00:00|2.9301939999999997|3.2793129999999997|         103.37425|          3.982049|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 09:00:00|2.4819592999999998|         2.7998161|         112.91657|         2.0289207|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 10:00:00|1.9674328999999997|         2.3240533|         117.71718|        0.98944813|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 11:00:00|         1.7692596|         2.1361778|         117.36697|        0.75310564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 12:00:00|1.7476258000000002|         2.1464431|         117.83322|         0.6366417|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 13:00:00|2.1857892999999997|         2.9506361|        119.379616|         0.6609495|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 14:00:00|2.3429406000000004|         3.6170475|         116.32994|0.6980888000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 15:00:00|         2.7623644|          4.243285|119.94583999999999|0.7787763000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 16:00:00|         2.9903412|4.7465269999999995|         122.55632|        0.98343766|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 17:00:00|         3.0250092|         5.0886664|        123.061676|1.0989118999999998|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 18:00:00|         3.0932174|         5.3077917|121.08698999999999|         1.4064169|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 19:00:00|         2.9959621|         5.2067146|         118.31918|         1.4490062|POINT (902000 457...|Campania|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Campania\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"NO2\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/campania/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133c8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 351:>                                                        (0 + 0) / 1]\r",
      "\r",
      "[Stage 351:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+---------+\n",
      "| original_date_time|             c_NO2|       x|        y|\n",
      "+-------------------+------------------+--------+---------+\n",
      "|2021-05-21 00:00:00|         2.4104354|902000.0|4576000.0|\n",
      "|2021-05-21 01:00:00|2.6902790000000003|902000.0|4576000.0|\n",
      "|2021-05-21 02:00:00|2.9086206000000003|902000.0|4576000.0|\n",
      "|2021-05-21 03:00:00|         3.2857485|902000.0|4576000.0|\n",
      "|2021-05-21 04:00:00|            2.8583|902000.0|4576000.0|\n",
      "|2021-05-21 05:00:00|         3.1135564|902000.0|4576000.0|\n",
      "|2021-05-21 06:00:00|          4.102874|902000.0|4576000.0|\n",
      "|2021-05-21 07:00:00|          4.456797|902000.0|4576000.0|\n",
      "|2021-05-21 08:00:00|          3.982049|902000.0|4576000.0|\n",
      "|2021-05-21 09:00:00|         2.0289207|902000.0|4576000.0|\n",
      "|2021-05-21 10:00:00|        0.98944813|902000.0|4576000.0|\n",
      "|2021-05-21 11:00:00|        0.75310564|902000.0|4576000.0|\n",
      "|2021-05-21 12:00:00|         0.6366417|902000.0|4576000.0|\n",
      "|2021-05-21 13:00:00|         0.6609495|902000.0|4576000.0|\n",
      "|2021-05-21 14:00:00|0.6980888000000001|902000.0|4576000.0|\n",
      "|2021-05-21 15:00:00|0.7787763000000001|902000.0|4576000.0|\n",
      "|2021-05-21 16:00:00|        0.98343766|902000.0|4576000.0|\n",
      "|2021-05-21 17:00:00|1.0989118999999998|902000.0|4576000.0|\n",
      "|2021-05-21 18:00:00|         1.4064169|902000.0|4576000.0|\n",
      "|2021-05-21 19:00:00|         1.4490062|902000.0|4576000.0|\n",
      "+-------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = df.withColumnRenamed(\"time\", \"original_date_time\")\n",
    "\n",
    "data_NO2 = data.select(col(\"original_date_time\"),col(\"c_NO2\"), col(\"x\"), col(\"y\"))\n",
    "data_NO2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd8d3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08067f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "train = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d99306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 352:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+---------+\n",
      "| original_date_time|             c_NO2|       x|        y|\n",
      "+-------------------+------------------+--------+---------+\n",
      "|2022-01-24 00:00:00|1.9606656000000002|902000.0|4576000.0|\n",
      "|2022-01-24 01:00:00|         2.2503803|902000.0|4576000.0|\n",
      "|2022-01-24 02:00:00|         2.8699071|902000.0|4576000.0|\n",
      "|2022-01-24 03:00:00|         2.9180806|902000.0|4576000.0|\n",
      "|2022-01-24 04:00:00|         2.6839666|902000.0|4576000.0|\n",
      "|2022-01-24 05:00:00|2.5214117000000003|902000.0|4576000.0|\n",
      "|2022-01-24 06:00:00|         2.5157216|902000.0|4576000.0|\n",
      "|2022-01-24 07:00:00|          3.228714|902000.0|4576000.0|\n",
      "|2022-01-24 08:00:00|         5.5078025|902000.0|4576000.0|\n",
      "|2022-01-24 09:00:00| 7.891017999999999|902000.0|4576000.0|\n",
      "|2022-01-24 10:00:00|          8.323253|902000.0|4576000.0|\n",
      "|2022-01-24 11:00:00|          5.982081|902000.0|4576000.0|\n",
      "|2022-01-24 12:00:00|3.6568059999999996|902000.0|4576000.0|\n",
      "|2022-01-24 13:00:00|         2.7666984|902000.0|4576000.0|\n",
      "|2022-01-24 14:00:00|2.9878256000000003|902000.0|4576000.0|\n",
      "|2022-01-24 15:00:00|         3.5123703|902000.0|4576000.0|\n",
      "|2022-01-24 16:00:00|         4.0173144|902000.0|4576000.0|\n",
      "|2022-01-24 17:00:00| 4.451185700000001|902000.0|4576000.0|\n",
      "|2022-01-24 18:00:00|           3.60134|902000.0|4576000.0|\n",
      "|2022-01-24 19:00:00|         3.3887837|902000.0|4576000.0|\n",
      "+-------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3852b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "train = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train = train.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test = test.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"original_date_time\", \"c_NO2\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8e673",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "708cf6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  12704.420602560043\n",
      "Time taken for forecasting:  0.04222822189331055\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "# Filter the dataframe\n",
    "train_dt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_NO2\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_output_path = \"output/decision_tree_regression_model_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_output_path = \"output/decision_tree_regression_forecast_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_output_path):\n",
    "    os.makedirs(forecast_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train_dt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test_dt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "#predictions_dt.write.csv(forecast_output_path, mode='overwrite', header=True)\n",
    "\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636c532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "# Load the saved decision tree regression model\n",
    "model_dt = DecisionTreeRegressionModel.load(\"output/decision_tree_regression_model_NO2/\")\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff1a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 1.7851186078730767\n",
      "R2 (Train Data): 0.9278755876193243\n",
      "MSE (Train Data): 3.1866484441747107\n",
      "MAE (Train Data): 0.4235088535537719\n"
     ]
    }
   ],
   "source": [
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3496f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.092151092625431\n",
      "R2 (Prediction Data): 0.9336795492427963\n",
      "MSE (Prediction Data): 1.1927940091229225\n",
      "MAE (Prediction Data): 0.25016286377606817\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b1b45",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea0f92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.129681842117258\n",
      "R2 (Train Data): 0.9885471112692592\n",
      "MSE (Train Data): 4.535544748643955\n",
      "MAE (Train Data): 0.8878116045992104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  70.71618890762329\n",
      "Time taken for forecasting:  0.07727527618408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==========================================>            (70 + 20) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.380644155900868\n",
      "R2 (Prediction Data): 0.9906763125490702\n",
      "MSE (Prediction Data): 1.9061782852232214\n",
      "MAE (Prediction Data): 0.5793366482508635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:=======================================================>(89 + 1) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe\n",
    "train_dt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_NO2\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_model_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_forecast_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749e938",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74cd9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.4717839963260007\n",
      "R2 (Train Data): 0.966607416789149\n",
      "MSE (Train Data): 6.109716124493333\n",
      "MAE (Train Data): 0.8246369478400293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  208.15062880516052\n",
      "Time taken for forecasting:  0.06748723983764648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:======>                                                 (8 + 63) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.1958069080106926\n",
      "R2 (Prediction Data): 0.9734323966719441\n",
      "MSE (Prediction Data): 1.429954161246093\n",
      "MAE (Prediction Data): 0.42030032151818897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:====================>                                  (26 + 45) / 71]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe\n",
    "train_dt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_NO2\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619665b6",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5770c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 377:====================================================>(107 + 1) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.542704003994547\n",
      "R2 (Train Data): 0.9132776416463229\n",
      "MSE (Train Data): 12.550751659918996\n",
      "MAE (Train Data): 0.8759254994350124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  510.12919569015503\n",
      "Time taken for forecasting:  0.04951834678649902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 387:===================================================>   (69 + 4) / 73]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.715327467041918\n",
      "R2 (Prediction Data): 0.9143967875981633\n",
      "MSE (Prediction Data): 7.373003253272275\n",
      "MAE (Prediction Data): 0.5666196127718319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe\n",
    "train_dt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_NO2\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68f6fc",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53efbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  19422.318557739258\n",
      "Time taken for forecasting:  0.15227961540222168\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"output/random_forest_regression_model_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"output/random_forest_regression_forecast_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ede7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93acaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.7359570343072435\n",
      "R2 (Prediction Data): 0.8324440076989488\n",
      "MSE (Prediction Data): 3.0135468249607995\n",
      "MAE (Prediction Data): 0.6231582184936116\n"
     ]
    }
   ],
   "source": [
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a850d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 1.9903672872990763\n",
      "R2 (Train Data): 0.9103367277819807\n",
      "MSE (Train Data): 3.9615619383502834\n",
      "MAE (Train Data): 0.8671681622614028\n",
      "Time taken for training:  53005.783642053604\n",
      "Time taken for forecasting:  0.08213400840759277\n",
      "RMSE (Prediction Data): 1.3127473162431975\n",
      "R2 (Prediction Data): 0.9041826184911201\n",
      "MSE (Prediction Data): 1.7233055163037176\n",
      "MAE (Prediction Data): 0.6095885104616765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882defd",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46634319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.6372660305085085\n",
      "R2 (Train Data): 0.9665931352740899\n",
      "MSE (Train Data): 13.229704176691122\n",
      "MAE (Train Data): 1.861044024294266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  95.05261826515198\n",
      "Time taken for forecasting:  0.07273364067077637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:============================================>          (73 + 17) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.516231294973753\n",
      "R2 (Prediction Data): 0.9690311336543322\n",
      "MSE (Prediction Data): 6.331419929805288\n",
      "MAE (Prediction Data): 1.3170223604426392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e591e9",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fc3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.389887289772796\n",
      "R2 (Train Data): 0.9371942361422225\n",
      "MSE (Train Data): 11.491335837363152\n",
      "MAE (Train Data): 1.4500104922940646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  107.41175961494446\n",
      "Time taken for forecasting:  0.06923365592956543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=====>                                                  (7 + 64) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.6102717615642892\n",
      "R2 (Prediction Data): 0.9518242353581241\n",
      "MSE (Prediction Data): 2.592975146091359\n",
      "MAE (Prediction Data): 0.7769157567499969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:===========================>                           (35 + 36) / 71]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee588404",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf4e2402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.7897762655089258\n",
      "R2 (Train Data): 0.946222563814257\n",
      "MSE (Train Data): 7.782851611596928\n",
      "MAE (Train Data): 1.0769555000363473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  143.59054327011108\n",
      "Time taken for forecasting:  0.08371639251708984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.1775347869099164\n",
      "R2 (Prediction Data): 0.9449476527524827\n",
      "MSE (Prediction Data): 4.741657748202814\n",
      "MAE (Prediction Data): 0.8264623473309387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 426:==================================================>    (67 + 6) / 73]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d660c79",
   "metadata": {},
   "source": [
    "# Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c4b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.744631981385418\n",
      "R2 (Train Data): 0.8295031447862559\n",
      "MSE (Train Data): 7.533004713243644\n",
      "MAE (Train Data): 0.5253342436717571\n",
      "Time taken for training:  170082.58302617073\n",
      "Time taken for forecasting:  0.07254385948181152\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3e25814ce14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Load the saved predictions from the Parquet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mpredictions_gbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_gbt_output_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    299\u001b[0m                        int96RebaseMode=int96RebaseMode)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"output/gradient_boosted_regression_model_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"output/gradient_boosted_regression_forecast_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ecb187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.6525308940469952\n",
      "R2 (Prediction Data): 0.8481617481944239\n",
      "MSE (Prediction Data): 2.7308583557797608\n",
      "MAE (Prediction Data): 0.2550917274251042\n"
     ]
    }
   ],
   "source": [
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9321579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b54f2",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111dcd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.0036131162735735\n",
      "R2 (Train Data): 0.9898629096484373\n",
      "MSE (Train Data): 4.014465519703501\n",
      "MAE (Train Data): 0.746170227682357\n",
      "Time taken for training:  303.44708466529846\n",
      "Time taken for forecasting:  0.06231689453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 542:======================================>               (64 + 26) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3453460728354338\n",
      "R2 (Prediction Data): 0.9911469642194409\n",
      "MSE (Prediction Data): 1.8099560556937246\n",
      "MAE (Prediction Data): 0.4408248717738538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 542:===================================================>   (85 + 5) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "#model_gbt_output_path = \"output/gradient_boosted_regression_model_NO2/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "#forecast_gbt_output_path = \"output/gradient_boosted_regression_forecast_NO2/\"\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_model_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_forecast_NO2_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052c1ff",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd79697f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.2822042980180473\n",
      "R2 (Train Data): 0.9715332411316119\n",
      "MSE (Train Data): 5.20845645789205\n",
      "MAE (Train Data): 0.6534048362348036\n",
      "Time taken for training:  352.25724148750305\n",
      "Time taken for forecasting:  0.13773441314697266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 305:======================================================>(70 + 1) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.0514757543558402\n",
      "R2 (Prediction Data): 0.9794586591907469\n",
      "MSE (Prediction Data): 1.105601261998183\n",
      "MAE (Prediction Data): 0.26172801448745625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_NO2_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afd570",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7548f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 452:==========>                                          (22 + 86) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:41:43 WARN BlockManager: Asked to remove block rdd_1358_81, which does not exist\n",
      "23/06/20 13:41:43 WARN BlockManager: Asked to remove block rdd_1358_62, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_54, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_84, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_65, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_8, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_47, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_76, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_102, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_53, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_17, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_97, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_3, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_57, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_28, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_60, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 452:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_45, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_15, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_29, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_19, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_40, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_35, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_39, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_13, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_27, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_66, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_21, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_101, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_10, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_67, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_22, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_16, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_30, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_50, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_82, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_25, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_18, which does not exist\n",
      "23/06/20 13:41:44 WARN BlockManager: Asked to remove block rdd_1358_14, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_37, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_49, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_106, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_58, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_68, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_0, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_6, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_96, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_11, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 452:===================>                                 (39 + 69) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_7, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_41, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_34, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_36, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_59, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_4, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_51, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_52, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_61, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_55, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_42, which does not exist\n",
      "23/06/20 13:41:45 WARN BlockManager: Asked to remove block rdd_1358_87, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 472:==>                                                   (5 + 96) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_86, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_3, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_62, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_76, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_60, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_8, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_51, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_69, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_38, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_85, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_46, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_41, which does not exist\n",
      "23/06/20 13:42:00 WARN BlockManager: Asked to remove block rdd_1402_52, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_22, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_39, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_1, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_6, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 472:==========>                                          (22 + 86) / 108]\r",
      "\r",
      "[Stage 472:===============>                                     (31 + 77) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_31, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_97, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_5, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_56, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_44, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_33, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_40, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_49, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_7, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_12, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_64, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_65, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_53, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_107, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_13, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 472:================>                                    (33 + 75) / 108]\r",
      "\r",
      "[Stage 472:=================>                                   (36 + 72) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_57, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_24, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_73, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_20, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_99, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_11, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_18, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 472:==================>                                  (37 + 71) / 108]\r",
      "\r",
      "[Stage 472:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_36, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_45, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_23, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_2, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_21, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_35, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_10, which does not exist\n",
      "23/06/20 13:42:01 WARN BlockManager: Asked to remove block rdd_1402_30, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_81, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_26, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_43, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_16, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_48, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_95, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_102, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_66, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_59, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_42, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_34, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_50, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_61, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_9, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_32, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_25, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 472:===================>                                 (39 + 69) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_15, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_101, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_27, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_19, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_94, which does not exist\n",
      "23/06/20 13:42:02 WARN BlockManager: Asked to remove block rdd_1402_106, which does not exist\n",
      "23/06/20 13:42:03 WARN BlockManager: Asked to remove block rdd_1402_29, which does not exist\n",
      "23/06/20 13:42:03 WARN BlockManager: Asked to remove block rdd_1402_0, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.361141169671314\n",
      "R2 (Train Data): 0.9614783171900979\n",
      "MSE (Train Data): 5.57498762311682\n",
      "MAE (Train Data): 0.6266397290711946\n",
      "Time taken for training:  242.1389377117157\n",
      "Time taken for forecasting:  0.14626836776733398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.9100923017960023\n",
      "R2 (Prediction Data): 0.9576401566301517\n",
      "MSE (Prediction Data): 3.6484526013803498\n",
      "MAE (Prediction Data): 0.39355104067590063\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_NO2.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_NO2.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_NO2\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_NO2', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_NO2_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_NO2\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0ae91",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning - all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b5cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "|original_date_time|    c_NO2|    x|     y|            features|    prediction_gbt|\n",
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "|       1.6447068E9|2.5869327|266.0|4016.0|[1.6447068E9,266....|2.4422328557171817|\n",
      "|       1.6447104E9|2.7651503|266.0|4016.0|[1.6447104E9,266....|2.9023172658082568|\n",
      "|        1.644714E9|2.6841903|266.0|4016.0|[1.644714E9,266.0...|2.9023172658082568|\n",
      "|       1.6447176E9| 2.259535|266.0|4016.0|[1.6447176E9,266....|2.3396340579003536|\n",
      "|       1.6447212E9|2.0554266|266.0|4016.0|[1.6447212E9,266....| 1.981799000029514|\n",
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+------+------------------+\n",
      "|original_date_time|     c_NO2|    x|     y|        prediction|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "|       1.6250904E9| 1.0153337|270.0|4696.0| 1.168626958046316|\n",
      "|       1.6250904E9| 2.1069567|270.0|5092.0|1.9863005799643687|\n",
      "|       1.6250904E9|0.69421554|282.0|4284.0|0.8573913412264741|\n",
      "|       1.6250904E9|0.36460355|290.0|4876.0|0.6688456490440449|\n",
      "|       1.6250904E9|0.51159096|294.0|5044.0|0.7420338262051449|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:(96 + 55) / 151][Stage 106:(0 + 41) / 155][Stage 107:>(0 + 0) / 155] \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 20:19:56 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:239)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:230)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@cresco6-nvi1.portici.enea.it:14104\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/05/07 20:19:56 WARN BlockManagerMasterEndpoint: Error trying to remove shuffle 44. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:14104 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:14104 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:(133 + 18) / 151][Stage 106:(0 + 78) / 155][Stage 107:>(0 + 0) / 155]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 20:21:03 WARN NettyRpcEnv: Ignored message: true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:(139 + 12) / 151][Stage 122:(0 + 84) / 155][Stage 123:>(0 + 0) / 155]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 23:24:32 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:239)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:230)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@cresco6-nvi1.portici.enea.it:14104\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/05/07 23:24:32 WARN BlockManagerMasterEndpoint: Error trying to remove shuffle 52. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:14104 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:14104 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:(142 + 9) / 151][Stage 122:(0 + 87) / 155][Stage 123:>(0 + 0) / 155]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 23:24:46 WARN NettyRpcEnv: Ignored message: true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=======================================>               (15 + 6) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.0598354366875957\n",
      "R2 (Prediction Data): 0.9375461866445275\n",
      "MSE (Prediction Data): 1.1232511528587867\n",
      "MAE (Prediction Data): 0.2734319495730649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 128:=================================================>     (19 + 2) / 21]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens = (\n",
    "    spark.read.parquet(\"output/decision_tree_regression_forecast_NO2/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens = (\n",
    "    spark.read.parquet(\"output/gradient_boosted_regression_forecast_NO2/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions = (\n",
    "    predictions_dt_ens\n",
    "    .join(predictions_rf_ens, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .join(predictions_gbt_ens, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_NO2\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions.show(5)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58fb55f",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5da9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6390908E9|          48.67309|462000.0|5016000.0|[1.6390908E9,4620...|50.667555291103966|\n",
      "|       1.6390944E9|           40.7803|462000.0|5016000.0|[1.6390944E9,4620...|  43.6108744390497|\n",
      "|        1.639098E9|31.483090999999998|462000.0|5016000.0|[1.639098E9,46200...| 31.06613807416743|\n",
      "|       1.6391016E9|         27.147745|462000.0|5016000.0|[1.6391016E9,4620...|  26.5198320954394|\n",
      "|       1.6391052E9|         22.509853|462000.0|5016000.0|[1.6391052E9,4620...|22.748608794187508|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|         1.9546407|470000.0|4996000.0| 2.287312269034557|\n",
      "|       1.6250904E9|1.1796991000000001|470000.0|5080000.0|1.8219501918561694|\n",
      "|       1.6250904E9|         2.1294494|474000.0|5000000.0| 2.406983499647844|\n",
      "|       1.6250904E9|3.2667450000000002|490000.0|5048000.0|  3.50956623082342|\n",
      "|       1.6250904E9|         1.7189535|490000.0|5064000.0|2.3399515155087456|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:=====================================================> (41 + 1) / 42]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.417739783170022\n",
      "R2 (Prediction Data): 0.9901685540902437\n",
      "MSE (Prediction Data): 2.0099860927829813\n",
      "MAE (Prediction Data): 0.5806174830027424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_NO2_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_NO2_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lombardia = (\n",
    "    predictions_dt_ens_lombardia\n",
    "    .join(predictions_rf_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .join(predictions_gbt_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_NO2\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lombardia.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lambordia = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lambordia}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lambordia}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lambordia}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lambordia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5837bc",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d153a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6295832E9|         0.9852556|702000.0|4696000.0|[1.6295832E9,7020...|0.9215848562779532|\n",
      "|       1.6295868E9|         1.0865685|702000.0|4696000.0|[1.6295868E9,7020...|1.1890336825690981|\n",
      "|       1.6295904E9|         1.1866157|702000.0|4696000.0|[1.6295904E9,7020...|1.1890336825690981|\n",
      "|        1.629594E9|          1.341053|702000.0|4696000.0|[1.629594E9,70200...|1.4421165517566346|\n",
      "|       1.6295976E9|1.4931348999999998|702000.0|4696000.0|[1.6295976E9,7020...|1.4421165517566346|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|         16.430355|730000.0|4672000.0|15.459802389272149|\n",
      "|       1.6250904E9|        0.78879404|738000.0|4712000.0|1.0250354121705338|\n",
      "|       1.6250904E9|        0.62703705|742000.0|4716000.0|0.9449435759518402|\n",
      "|       1.6250904E9|1.9981311999999998|746000.0|4688000.0|1.8161156602470498|\n",
      "|       1.6250904E9|2.0688883999999996|750000.0|4676000.0|1.8668389378958785|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 348:========================>                            (47 + 53) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 0.89771834134351\n",
      "R2 (Prediction Data): 0.9850269365982262\n",
      "MSE (Prediction Data): 0.8058982203845427\n",
      "MAE (Prediction Data): 0.3287325085704986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 348:===================================================>  (96 + 4) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_NO2_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_NO2_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lazio.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lazio = (\n",
    "    predictions_dt_ens_lazio\n",
    "    .join(predictions_rf_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .join(predictions_gbt_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_NO2\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lazio.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lazio = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lazio}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lazio}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lazio}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lazio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe243d0",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f70a305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6429788E9|1.9606656000000002|902000.0|4576000.0|[1.6429788E9,9020...| 2.055802587153065|\n",
      "|       1.6429824E9|         2.2503803|902000.0|4576000.0|[1.6429824E9,9020...| 2.196134549756191|\n",
      "|        1.642986E9|         2.8699071|902000.0|4576000.0|[1.642986E9,90200...|3.0327243657123684|\n",
      "|       1.6429896E9|         2.9180806|902000.0|4576000.0|[1.6429896E9,9020...|3.0327243657123684|\n",
      "|       1.6429932E9|         2.6839666|902000.0|4576000.0|[1.6429932E9,9020...| 2.535548000284666|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|             c_NO2|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|1.9462897000000001|910000.0|4596000.0|2.0189383122784377|\n",
      "|       1.6250904E9|         1.3335419|926000.0|4564000.0|1.6479732550474668|\n",
      "|       1.6250904E9|         11.416698|958000.0|4528000.0|11.119329204422376|\n",
      "|       1.6250904E9|         1.9531003|978000.0|4556000.0|2.2494949954853163|\n",
      "|       1.6250904E9|         2.4629998|982000.0|4564000.0|2.5660140296276315|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 698:=========================================>           (79 + 21) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.8325053248159318\n",
      "R2 (Prediction Data): 0.9610115358505835\n",
      "MSE (Prediction Data): 3.358075765478745\n",
      "MAE (Prediction Data): 0.42227832800529214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_NO2_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_NO2_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_NO2_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_campania.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_campania = (\n",
    "    predictions_dt_ens_campania\n",
    "    .join(predictions_rf_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .join(predictions_gbt_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_NO2\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_NO2\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_campania.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_campania = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_NO2\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_campania}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_campania}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_campania}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_campania}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d09246f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a35e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
