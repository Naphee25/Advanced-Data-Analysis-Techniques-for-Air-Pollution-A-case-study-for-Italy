{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a952098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 19:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 19:07:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/05/07 19:08:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/07 19:08:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/07 19:08:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM25: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"O3\").config(\"spark.driver.memory\", \"16g\").config(\"spark.executor.memory\", \"8g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.driver.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.executor.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"../dataset/csv/\", header=True, schema=schema)\n",
    "\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2')\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0e3d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 19:18:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 19:18:37 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|  DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|462000.0|5016000.0|20.0|2019-11-24 00:00:00|         1.6957362|         2.7321966|          50.69704| 4.208417400000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 01:00:00|1.5195319999999999|         2.5212698|         52.217495|3.6308307999999996|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 02:00:00|1.4698639999999998|          2.540143|53.528316000000004|         3.0628326|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 03:00:00|         1.4075043|         2.5400522|         55.269703|          2.444656|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 04:00:00|1.3243808000000001|         2.4868004| 56.09563000000001|         1.9309062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 05:00:00|1.2917066000000001|         2.4511726|56.588367000000005|         1.6992023|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 06:00:00|         1.3729354|         2.5612066|          56.55035|1.5480975000000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 07:00:00|         1.8005681|         2.9130769|          53.39305|         2.1196866|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 08:00:00|         2.7273355|         3.6733246|         45.281013|         4.0332394|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 09:00:00|4.3827419999999995|         5.3303866|         39.011024|         7.3524475|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 10:00:00| 4.251999400000001|         5.3588486|         42.592384|          6.889144|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 11:00:00|3.8547763999999995|          4.963749|42.843109999999996|          6.416884|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 12:00:00|         3.8144705|         4.8981695|         41.092995|6.6946254000000005|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 13:00:00|         3.4577906| 4.584929499999999|42.440692999999996|          6.609909|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 14:00:00|          6.724634|           8.32361|         40.356117|         11.530877|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 15:00:00|         6.9248476| 8.738185000000001|         39.104855|         15.190662|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 16:00:00|         4.1298723|         5.5023584|          37.11807|         13.312062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 17:00:00|          2.274489|            3.0258|         39.018482|         7.7159667|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 18:00:00|         2.2451131|3.0147939999999998|         35.107414|          8.846022|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 19:00:00|         2.5813787|          3.473392|          33.05836|          9.195902|POINT (462000 501...|Lombardia|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lombardia\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"O3\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/data_chunk/lombardia/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f927a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/15 10:01:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/15 10:01:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/15 10:01:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/15 10:01:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|702000.0|4696000.0|20.0|2021-12-11 00:00:00|         1.2706414|1.5203451000000001|60.837047999999996|         2.0367115|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 01:00:00|          1.230251|          1.375211|         57.657654|         2.0870926|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 02:00:00|1.9736630000000002|          2.107126|          54.05386|          2.587756|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 03:00:00|1.7445994999999999|1.8961723000000001|          54.00688|         2.1309106|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 04:00:00|         1.2854077|1.5713663999999998|54.418915000000005|1.6039573999999999|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 05:00:00|0.8523575999999999|         1.1182661| 53.96324499999999|         1.8012519|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 06:00:00|        0.78199506|        0.94059986|         48.937576|          2.225234|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 07:00:00|         1.2314979|         1.4721483|46.245734999999996|         2.2615266|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 08:00:00|         2.3940308|         3.0101209|          51.23545|2.1766273999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 09:00:00|         3.0954514|3.8334242999999995|         51.540134|2.1541189999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 10:00:00|         2.8416717|3.5963568999999995|           54.0782|1.7994093999999998|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 11:00:00|3.1728162999999996|          3.890354|          54.80785|1.9321240000000002|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 12:00:00|         2.9845037|         3.5793805|52.454944999999995|         2.0442376|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 13:00:00|3.3172165999999996|         4.0856614|         52.290283|         2.3790326|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 14:00:00|         3.1035097|         3.9299242|52.074946999999995|2.6943377999999996|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 15:00:00|           2.73482|3.7316300000000004|         52.074814|2.7827827999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 16:00:00|2.6398816000000003|          3.834493|         54.961468|         2.8663425|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 17:00:00|         2.5602984|         3.5236087|          57.12959|          2.730953|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 18:00:00|2.3228847999999997|         3.1005373| 63.62944399999999|         2.3298986|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 19:00:00|         2.3371992|         3.0020676| 64.78435999999999|2.1526132000000002|POINT (702000 469...|  Lazio|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Lazio\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"O3\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/lazio/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3f1411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:27:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/20 13:27:52 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/06/20 13:28:00 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry| DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|902000.0|4576000.0|20.0|2021-05-21 00:00:00|2.5389232999999995|         2.9607124|         102.89293|         2.4104354|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 01:00:00|         2.5921166|         2.9856164| 98.94353000000001|2.6902790000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 02:00:00|2.5095107999999997|2.8129014999999997|102.21184000000001|2.9086206000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 03:00:00|         2.4757087|         2.7655613|          102.6552|         3.2857485|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 04:00:00|2.4431244999999997|          2.733494|          99.22969|            2.8583|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 05:00:00|          2.602668|         2.9043045|         91.063896|         3.1135564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 06:00:00|         2.9900506|3.3243476999999997|          86.42286|          4.102874|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 07:00:00|         3.1773403|          3.527025| 92.17261500000001|          4.456797|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 08:00:00|2.9301939999999997|3.2793129999999997|         103.37425|          3.982049|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 09:00:00|2.4819592999999998|         2.7998161|         112.91657|         2.0289207|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 10:00:00|1.9674328999999997|         2.3240533|         117.71718|        0.98944813|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 11:00:00|         1.7692596|         2.1361778|         117.36697|        0.75310564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 12:00:00|1.7476258000000002|         2.1464431|         117.83322|         0.6366417|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 13:00:00|2.1857892999999997|         2.9506361|        119.379616|         0.6609495|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 14:00:00|2.3429406000000004|         3.6170475|         116.32994|0.6980888000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 15:00:00|         2.7623644|          4.243285|119.94583999999999|0.7787763000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 16:00:00|         2.9903412|4.7465269999999995|         122.55632|        0.98343766|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 17:00:00|         3.0250092|         5.0886664|        123.061676|1.0989118999999998|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 18:00:00|         3.0932174|         5.3077917|121.08698999999999|         1.4064169|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 19:00:00|         2.9959621|         5.2067146|         118.31918|         1.4490062|POINT (902000 457...|Campania|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Campania\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"O3\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/campania/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acba4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+---------+\n",
      "| original_date_time|              c_O3|       x|        y|\n",
      "+-------------------+------------------+--------+---------+\n",
      "|2021-05-21 00:00:00|         102.89293|902000.0|4576000.0|\n",
      "|2021-05-21 01:00:00| 98.94353000000001|902000.0|4576000.0|\n",
      "|2021-05-21 02:00:00|102.21184000000001|902000.0|4576000.0|\n",
      "|2021-05-21 03:00:00|          102.6552|902000.0|4576000.0|\n",
      "|2021-05-21 04:00:00|          99.22969|902000.0|4576000.0|\n",
      "|2021-05-21 05:00:00|         91.063896|902000.0|4576000.0|\n",
      "|2021-05-21 06:00:00|          86.42286|902000.0|4576000.0|\n",
      "|2021-05-21 07:00:00| 92.17261500000001|902000.0|4576000.0|\n",
      "|2021-05-21 08:00:00|         103.37425|902000.0|4576000.0|\n",
      "|2021-05-21 09:00:00|         112.91657|902000.0|4576000.0|\n",
      "|2021-05-21 10:00:00|         117.71718|902000.0|4576000.0|\n",
      "|2021-05-21 11:00:00|         117.36697|902000.0|4576000.0|\n",
      "|2021-05-21 12:00:00|         117.83322|902000.0|4576000.0|\n",
      "|2021-05-21 13:00:00|        119.379616|902000.0|4576000.0|\n",
      "|2021-05-21 14:00:00|         116.32994|902000.0|4576000.0|\n",
      "|2021-05-21 15:00:00|119.94583999999999|902000.0|4576000.0|\n",
      "|2021-05-21 16:00:00|         122.55632|902000.0|4576000.0|\n",
      "|2021-05-21 17:00:00|        123.061676|902000.0|4576000.0|\n",
      "|2021-05-21 18:00:00|121.08698999999999|902000.0|4576000.0|\n",
      "|2021-05-21 19:00:00|         118.31918|902000.0|4576000.0|\n",
      "+-------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = df.withColumnRenamed(\"time\", \"original_date_time\")\n",
    "\n",
    "data_O3 = data.select(col(\"original_date_time\"),col(\"c_O3\"), col(\"x\"), col(\"y\"))\n",
    "data_O3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d69f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b546da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18913548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>(107 + 1) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7583652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "train_count = train.count()\n",
    "print(train_count)\n",
    "test_count = test.count()\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de56b4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique x count: 45\n",
      "Unique y count: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# assuming your dataframe is called \"df\"\n",
    "unique_utm_count = data_O3.agg(countDistinct(\"x\"), countDistinct(\"y\")).collect()\n",
    "\n",
    "# print the result\n",
    "print(\"Unique x count:\", unique_utm_count[0][0])\n",
    "print(\"Unique y count:\", unique_utm_count[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942e6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "train = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train = train.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test = test.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2753515",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc51de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  15388.484159469604\n",
      "Time taken for forecasting:  0.4566805362701416\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_dt = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_output_path = \"output/decision_tree_regression_model_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_output_path = \"output/decision_tree_regression_forecast_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_output_path):\n",
    "    os.makedirs(forecast_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train_dt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test_dt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "#predictions_dt.write.csv(forecast_output_path, mode='overwrite', header=True)\n",
    "\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3ebeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "# Load the saved decision tree regression model\n",
    "model_dt = DecisionTreeRegressionModel.load(\"output/decision_tree_regression_model_O3/\")\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41b31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.486420600629922\n",
      "R2 (Train Data): 0.9885401681582298\n",
      "MSE (Train Data): 6.182287403236862\n",
      "MAE (Train Data): 1.3245126524097572\n"
     ]
    }
   ],
   "source": [
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cbaf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.121446818579103\n",
      "R2 (Prediction Data): 0.989360709447123\n",
      "MSE (Prediction Data): 4.5005366040593975\n",
      "MAE (Prediction Data): 1.1474509466779057\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf121ba9",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea6a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.31099792104079\n",
      "R2 (Train Data): 0.9944602083435302\n",
      "MSE (Train Data): 5.340711391054852\n",
      "MAE (Train Data): 1.3948318310763124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  102.28956532478333\n",
      "Time taken for forecasting:  0.11788177490234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=====================>                                 (35 + 55) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.7540975126611094\n",
      "R2 (Prediction Data): 0.9830220311194097\n",
      "MSE (Prediction Data): 14.093248134568327\n",
      "MAE (Prediction Data): 1.6267858494140122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:=======================================>               (65 + 25) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_model_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_forecast_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba13588",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3c0187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.4371727395521843\n",
      "R2 (Train Data): 0.9888906782340222\n",
      "MSE (Train Data): 5.9398109624163\n",
      "MAE (Train Data): 1.3322115991252355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  570.5779016017914\n",
      "Time taken for forecasting:  0.08865141868591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:====================================================>   (67 + 4) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.5845886503411766\n",
      "R2 (Prediction Data): 0.9827035234568476\n",
      "MSE (Prediction Data): 6.680098491472424\n",
      "MAE (Prediction Data): 1.216089463061496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962b934",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4b5d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.7615087873093542\n",
      "R2 (Train Data): 0.9822515074089758\n",
      "MSE (Train Data): 7.625930782386781\n",
      "MAE (Train Data): 1.367557592886549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  48.74492621421814\n",
      "Time taken for forecasting:  0.19266414642333984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.0199161854139875\n",
      "R2 (Prediction Data): 0.9751342983982529\n",
      "MSE (Prediction Data): 9.119893766925369\n",
      "MAE (Prediction Data): 1.3628242633050762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1719a",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75835952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  14206.954404354095\n",
      "Time taken for forecasting:  0.1753525733947754\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5, numTrees=10)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"output/random_forest_regression_model_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"output/random_forest_regression_forecast_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8eb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 5.365972819965525\n",
      "R2 (Prediction Data): 0.9319316366980714\n",
      "MSE (Prediction Data): 28.793664304608768\n",
      "MAE (Prediction Data): 3.1107171796655755\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfd616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 4.567896237903502\n",
      "R2 (Train Data): 0.9613222221663525\n",
      "MSE (Train Data): 20.86567604025297\n",
      "MAE (Train Data): 2.624279573687162\n",
      "Time taken for training:  52918.29840755463\n",
      "Time taken for forecasting:  0.09033989906311035\n",
      "RMSE (Prediction Data): 4.343202386916817\n",
      "R2 (Prediction Data): 0.9554068136165021\n",
      "MSE (Prediction Data): 18.863406973719936\n",
      "MAE (Prediction Data): 2.472916371100026\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01567113",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa4333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 7.314815796660012\n",
      "R2 (Train Data): 0.9444989613691692\n",
      "MSE (Train Data): 53.50653013906686\n",
      "MAE (Train Data): 5.090758056773691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  153.37667155265808\n",
      "Time taken for forecasting:  0.0938417911529541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:=======================>                              (39 + 51) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 9.71530898389048\n",
      "R2 (Prediction Data): 0.8862928250829548\n",
      "MSE (Prediction Data): 94.3872286524631\n",
      "MAE (Prediction Data): 6.771553602940127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 104:=====================================================> (88 + 2) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91720647",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47584733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 5.909746415513379\n",
      "R2 (Train Data): 0.934679031704703\n",
      "MSE (Train Data): 34.92510269567324\n",
      "MAE (Train Data): 3.58837663354881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  249.0763020515442\n",
      "Time taken for forecasting:  0.1004185676574707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:=====================================================>  (68 + 3) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 6.928040505962635\n",
      "R2 (Prediction Data): 0.8757216115990364\n",
      "MSE (Prediction Data): 47.997745252259\n",
      "MAE (Prediction Data): 4.676939394072467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443c48",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce538432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 4.900029593476857\n",
      "R2 (Train Data): 0.9441187618095866\n",
      "MSE (Train Data): 24.01029001694897\n",
      "MAE (Train Data): 2.9081629913142364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  98.71785235404968\n",
      "Time taken for forecasting:  0.13309693336486816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 5.2522253028990535\n",
      "R2 (Prediction Data): 0.9247861822625992\n",
      "MSE (Prediction Data): 27.58587063241305\n",
      "MAE (Prediction Data): 2.9363433012014717\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_O3\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ac6dd",
   "metadata": {},
   "source": [
    "# Gradient boosting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d7fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.4890996047484752\n",
      "R2 (Train Data): 0.9885154599683768\n",
      "MSE (Train Data): 6.1956168423590166\n",
      "MAE (Train Data): 1.1154500845091744\n",
      "Time taken for training:  170857.71802592278\n",
      "Time taken for forecasting:  0.07757425308227539\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1f3d5774e265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Load the saved predictions from the Parquet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mpredictions_gbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_gbt_output_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Create a Regression Evaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    299\u001b[0m                        int96RebaseMode=int96RebaseMode)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_O3\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"output/gradient_boosted_regression_model_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"output/gradient_boosted_regression_forecast_O3/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fffeb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.23495353964439\n",
      "R2 (Prediction Data): 0.9881917546048439\n",
      "MSE (Prediction Data): 4.99501732436899\n",
      "MAE (Prediction Data): 0.9439706651622005\n"
     ]
    }
   ],
   "source": [
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf508a34",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f694dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.1831680733901893\n",
      "R2 (Train Data): 0.995056111523329\n",
      "MSE (Train Data): 4.76622283667023\n",
      "MAE (Train Data): 1.126561528592175\n",
      "Time taken for training:  813.1202802658081\n",
      "Time taken for forecasting:  0.1860053539276123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:===================================================>   (84 + 6) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.7377267854420015\n",
      "R2 (Prediction Data): 0.9831697820382359\n",
      "MSE (Prediction Data): 13.970601522610597\n",
      "MAE (Prediction Data): 1.321034937747513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 235:=====================================================> (88 + 2) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_O3\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/outputgradient_boosted_regression_model_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_forecast_O3_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49feb85b",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1546cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.107593537591581\n",
      "R2 (Train Data): 0.9916921501535798\n",
      "MSE (Train Data): 4.441950519697796\n",
      "MAE (Train Data): 1.0881862439051082\n",
      "Time taken for training:  632.877650976181\n",
      "Time taken for forecasting:  0.0835123062133789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 316:=========================================>            (55 + 16) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.1091719904915593\n",
      "R2 (Prediction Data): 0.9884814246640309\n",
      "MSE (Prediction Data): 4.4486064854741265\n",
      "MAE (Prediction Data): 0.9823942372951976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_O3\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/outputgradient_boosted_regression_model_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_O3_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7c783",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c89a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:==================>                                  (37 + 71) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_69, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_24, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_23, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_9, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_0, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_57, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_56, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_54, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_58, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_11, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_7, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_1, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_20, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_104, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_34, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_21, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_82, which does not exist\n",
      "23/06/20 13:44:58 WARN BlockManager: Asked to remove block rdd_532_5, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_96, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_70, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_46, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_13, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_103, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_32, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_17, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_3, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_94, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_55, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_63, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_39, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 193:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_27, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_10, which does not exist\n",
      "23/06/20 13:44:59 WARN BlockManager: Asked to remove block rdd_532_19, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 233:===>                                                  (6 + 96) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:24 WARN BlockManager: Asked to remove block rdd_620_105, which does not exist\n",
      "23/06/20 13:46:24 WARN BlockManager: Asked to remove block rdd_620_5, which does not exist\n",
      "23/06/20 13:46:24 WARN BlockManager: Asked to remove block rdd_620_58, which does not exist\n",
      "23/06/20 13:46:24 WARN BlockManager: Asked to remove block rdd_620_39, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_6, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_11, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_70, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:=====>                                               (12 + 96) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_63, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_78, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_75, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_12, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_101, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_16, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:======>                                              (13 + 95) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_46, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_3, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_52, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_55, which does not exist\n",
      "23/06/20 13:46:25 WARN BlockManager: Asked to remove block rdd_620_104, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_99, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_69, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_107, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_53, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_88, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_17, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_25, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_4, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_57, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_41, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_77, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_82, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_23, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_90, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_89, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_26, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_42, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_91, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_13, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_102, which does not exist\n",
      "23/06/20 13:46:26 WARN BlockManager: Asked to remove block rdd_620_45, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:======>                                              (14 + 94) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_85, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_21, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_19, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_51, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_33, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:=======>                                             (15 + 93) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_35, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_18, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_2, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_94, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_37, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_83, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_81, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_24, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_8, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_36, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_68, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_20, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_95, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_79, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_40, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_87, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_15, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_64, which does not exist\n",
      "23/06/20 13:46:27 WARN BlockManager: Asked to remove block rdd_620_47, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_65, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_93, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_67, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:=======>                                             (16 + 92) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_30, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_38, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_44, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_73, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_1, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_61, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_54, which does not exist\n",
      "23/06/20 13:46:28 WARN BlockManager: Asked to remove block rdd_620_0, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_9, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_64, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_60, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_57, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_21, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_37, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_104, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_3, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_49, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_75, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_32, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_4, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_47, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_74, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_48, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_38, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_5, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_14, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_35, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_55, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_52, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_54, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_99, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_66, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_58, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_22, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_80, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_93, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_12, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_62, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_59, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_61, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_50, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_23, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_18, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_33, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_68, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_84, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_17, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_45, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_46, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_13, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_102, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_6, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_19, which does not exist\n",
      "23/06/20 13:47:19 WARN BlockManager: Asked to remove block rdd_664_29, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.9256616947161826\n",
      "R2 (Train Data): 0.9800787389867217\n",
      "MSE (Train Data): 8.559496351929564\n",
      "MAE (Train Data): 1.1585673841450241\n",
      "Time taken for training:  314.4399025440216\n",
      "Time taken for forecasting:  0.09544944763183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.5189255154500856\n",
      "R2 (Prediction Data): 0.9662377729055363\n",
      "MSE (Prediction Data): 12.382836783285649\n",
      "MAE (Prediction Data): 1.1581039696015212\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_O3.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_O3.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_O3\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_O3', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/outputgradient_boosted_regression_model_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_O3_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_O3\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560bf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcc791",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e940cfd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "|original_date_time|     c_O3|    x|     y|            features|   prediction_gbt|\n",
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "|       1.6446204E9| 80.73667|266.0|4016.0|[1.6446204E9,266....|80.67768412528541|\n",
      "|        1.644624E9| 77.47521|266.0|4016.0|[1.644624E9,266.0...|77.43619099505625|\n",
      "|       1.6446276E9| 73.18715|266.0|4016.0|[1.6446276E9,266....|73.90708460893836|\n",
      "|       1.6446312E9|70.454704|266.0|4016.0|[1.6446312E9,266....|69.84530581599807|\n",
      "|       1.6446348E9|67.711205|266.0|4016.0|[1.6446348E9,266....|68.43009166775562|\n",
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+------+------------------+\n",
      "|original_date_time|      c_O3|    x|     y|        prediction|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "|       1.6250904E9| 105.42844|266.0|4464.0|104.70628061718656|\n",
      "|       1.6250904E9| 108.20571|270.0|4336.0|107.36635740345709|\n",
      "|       1.6250904E9| 105.34896|270.0|4416.0|104.70628061718656|\n",
      "|       1.6250904E9|  93.47508|270.0|5124.0|  93.3197943811307|\n",
      "|       1.6250904E9|107.260826|274.0|4700.0|107.45939129745899|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:====================================================>(494 + 6) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.357608985479972\n",
      "R2 (Prediction Data): 0.9868601005301902\n",
      "MSE (Prediction Data): 5.558320128415904\n",
      "MAE (Prediction Data): 1.0124105323500365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens = (\n",
    "    spark.read.parquet(\"output/decision_tree_regression_forecast_O3/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_O3/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens = (\n",
    "    spark.read.parquet(\"output/gradient_boosted_regression_forecast_O3/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions = (\n",
    "    predictions_dt_ens\n",
    "    .join(predictions_rf_ens, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .join(predictions_gbt_ens, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_O3\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions.show(5)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0b171",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d8f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|              c_O3|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6390908E9|          9.930322|462000.0|5016000.0|[1.6390908E9,4620...|13.769170461014822|\n",
      "|       1.6390944E9|         20.855484|462000.0|5016000.0|[1.6390944E9,4620...|21.527804062123487|\n",
      "|        1.639098E9|30.183638000000002|462000.0|5016000.0|[1.639098E9,46200...|28.031469776604688|\n",
      "|       1.6391016E9|          35.36551|462000.0|5016000.0|[1.6391016E9,4620...|33.386415044695525|\n",
      "|       1.6391052E9|39.964527000000004|462000.0|5016000.0|[1.6391052E9,4620...| 41.16933727821904|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|              c_O3|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|108.91788000000001|470000.0|5008000.0|106.03408577954137|\n",
      "|       1.6250904E9|        107.845375|478000.0|5016000.0|106.03666624303112|\n",
      "|       1.6250904E9|         109.06426|482000.0|5016000.0|106.03666624303112|\n",
      "|       1.6250904E9|114.43911999999999|486000.0|4992000.0|119.03177236609473|\n",
      "|       1.6250904E9|          99.23232|486000.0|5040000.0| 99.23212458213192|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:======================================================> (45 + 1) / 46]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 4.69527864624267\n",
      "R2 (Prediction Data): 0.9734418575876123\n",
      "MSE (Prediction Data): 22.045641565862404\n",
      "MAE (Prediction Data): 2.5854913808303572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_O3_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_O3_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_O3_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lombardia = (\n",
    "    predictions_dt_ens_lombardia\n",
    "    .join(predictions_rf_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .join(predictions_gbt_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_O3\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lombardia.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lambordia = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lambordia}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lambordia}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lambordia}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lambordia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee571b",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2a0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "|original_date_time|             c_O3|       x|        y|            features|   prediction_gbt|\n",
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "|       1.6295832E9|         99.60451|702000.0|4696000.0|[1.6295832E9,7020...|98.23231856713588|\n",
      "|       1.6295868E9|         97.50509|702000.0|4696000.0|[1.6295868E9,7020...|98.23231856713588|\n",
      "|       1.6295904E9|92.72381999999999|702000.0|4696000.0|[1.6295904E9,7020...|91.52499712362423|\n",
      "|        1.629594E9|         88.99645|702000.0|4696000.0|[1.629594E9,70200...| 88.8308963843096|\n",
      "|       1.6295976E9|         85.12988|702000.0|4696000.0|[1.6295976E9,7020...|84.43106693288205|\n",
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+--------+---------+-----------------+\n",
      "|original_date_time|             c_O3|       x|        y|       prediction|\n",
      "+------------------+-----------------+--------+---------+-----------------+\n",
      "|       1.6250904E9|97.45298000000001|718000.0|4712000.0|97.04641758447782|\n",
      "|       1.6250904E9|         94.06147|726000.0|4712000.0|93.54190078055852|\n",
      "|       1.6250904E9|        97.628975|734000.0|4724000.0|97.04641758447782|\n",
      "|       1.6250904E9|         99.87791|734000.0|4728000.0|98.48209539287909|\n",
      "|       1.6250904E9|         96.05501|754000.0|4704000.0|95.82452733480945|\n",
      "+------------------+-----------------+--------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 363:===========================>                         (52 + 48) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.9812044583665016\n",
      "R2 (Prediction Data): 0.9769877914850721\n",
      "MSE (Prediction Data): 8.887580022584306\n",
      "MAE (Prediction Data): 1.6572257278082418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 363:=====================================================>(99 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_O3_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_O3_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_O3_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lazio.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lazio = (\n",
    "    predictions_dt_ens_lazio\n",
    "    .join(predictions_rf_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .join(predictions_gbt_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_O3\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lazio.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lazio = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lazio}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lazio}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lazio}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lazio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19e3c0",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9f172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+-----------------+\n",
      "|original_date_time|              c_O3|       x|        y|            features|   prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+-----------------+\n",
      "|       1.6339032E9|          75.04703|902000.0|4576000.0|[1.6339032E9,9020...|75.92955140859998|\n",
      "|       1.6339068E9|         69.493996|902000.0|4576000.0|[1.6339068E9,9020...| 69.3095840141115|\n",
      "|       1.6339104E9|         63.977833|902000.0|4576000.0|[1.6339104E9,9020...|63.44196644652691|\n",
      "|        1.633914E9|62.321667000000005|902000.0|4576000.0|[1.633914E9,90200...|61.42126745203158|\n",
      "|       1.6339176E9|         60.875484|902000.0|4576000.0|[1.6339176E9,9020...|61.42126745203158|\n",
      "+------------------+------------------+--------+---------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|              c_O3|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|110.82198000000001|914000.0|4556000.0|110.12159457528269|\n",
      "|       1.6250904E9|         110.67533|934000.0|4536000.0|110.24356744873324|\n",
      "|       1.6250904E9|          104.3681|934000.0|4576000.0|104.74346718664793|\n",
      "|       1.6250904E9|108.78238999999999|970000.0|4568000.0|111.17533144207921|\n",
      "|       1.6250904E9|101.79366999999999|974000.0|4516000.0|100.04166022758113|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 402:=========================================>           (78 + 22) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.1044769782273822\n",
      "R2 (Prediction Data): 0.9737222751343778\n",
      "MSE (Prediction Data): 9.637777308343818\n",
      "MAE (Prediction Data): 1.148381992267082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_O3_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_O3_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_O3_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_campania.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_campania = (\n",
    "    predictions_dt_ens_campania\n",
    "    .join(predictions_rf_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .join(predictions_gbt_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_O3\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_O3\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_campania.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_campania = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_O3\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_campania}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_campania}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_campania}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_campania}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7ef8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bae877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
