{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ed720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 19:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 19:10:57 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM25: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM10\").config(\"spark.driver.memory\", \"16g\").config(\"spark.executor.memory\", \"8g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.driver.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.executor.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"../dataset/csv/\", header=True, schema=schema)\n",
    "\n",
    "#df = df.repartition(30)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2')\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f48e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 19:18:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 19:18:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/07 19:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|  DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|462000.0|5016000.0|20.0|2019-11-24 00:00:00|         1.6957362|         2.7321966|          50.69704| 4.208417400000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 01:00:00|1.5195319999999999|         2.5212698|         52.217495|3.6308307999999996|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 02:00:00|1.4698639999999998|          2.540143|53.528316000000004|         3.0628326|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 03:00:00|         1.4075043|         2.5400522|         55.269703|          2.444656|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 04:00:00|1.3243808000000001|         2.4868004| 56.09563000000001|         1.9309062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 05:00:00|1.2917066000000001|         2.4511726|56.588367000000005|         1.6992023|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 06:00:00|         1.3729354|         2.5612066|          56.55035|1.5480975000000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 07:00:00|         1.8005681|         2.9130769|          53.39305|         2.1196866|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 08:00:00|         2.7273355|         3.6733246|         45.281013|         4.0332394|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 09:00:00|4.3827419999999995|         5.3303866|         39.011024|         7.3524475|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 10:00:00| 4.251999400000001|         5.3588486|         42.592384|          6.889144|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 11:00:00|3.8547763999999995|          4.963749|42.843109999999996|          6.416884|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 12:00:00|         3.8144705|         4.8981695|         41.092995|6.6946254000000005|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 13:00:00|         3.4577906| 4.584929499999999|42.440692999999996|          6.609909|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 14:00:00|          6.724634|           8.32361|         40.356117|         11.530877|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 15:00:00|         6.9248476| 8.738185000000001|         39.104855|         15.190662|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 16:00:00|         4.1298723|         5.5023584|          37.11807|         13.312062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 17:00:00|          2.274489|            3.0258|         39.018482|         7.7159667|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 18:00:00|         2.2451131|3.0147939999999998|         35.107414|          8.846022|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 19:00:00|         2.5813787|          3.473392|          33.05836|          9.195902|POINT (462000 501...|Lombardia|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lombardia\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM10\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/data_chunk/lombardia/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7872f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/15 10:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/15 10:41:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/15 10:41:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/15 10:41:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/15 10:41:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|702000.0|4696000.0|20.0|2021-12-11 00:00:00|         1.2706414|1.5203451000000001|60.837047999999996|         2.0367115|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 01:00:00|          1.230251|          1.375211|         57.657654|         2.0870926|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 02:00:00|1.9736630000000002|          2.107126|          54.05386|          2.587756|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 03:00:00|1.7445994999999999|1.8961723000000001|          54.00688|         2.1309106|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 04:00:00|         1.2854077|1.5713663999999998|54.418915000000005|1.6039573999999999|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 05:00:00|0.8523575999999999|         1.1182661| 53.96324499999999|         1.8012519|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 06:00:00|        0.78199506|        0.94059986|         48.937576|          2.225234|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 07:00:00|         1.2314979|         1.4721483|46.245734999999996|         2.2615266|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 08:00:00|         2.3940308|         3.0101209|          51.23545|2.1766273999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 09:00:00|         3.0954514|3.8334242999999995|         51.540134|2.1541189999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 10:00:00|         2.8416717|3.5963568999999995|           54.0782|1.7994093999999998|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 11:00:00|3.1728162999999996|          3.890354|          54.80785|1.9321240000000002|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 12:00:00|         2.9845037|         3.5793805|52.454944999999995|         2.0442376|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 13:00:00|3.3172165999999996|         4.0856614|         52.290283|         2.3790326|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 14:00:00|         3.1035097|         3.9299242|52.074946999999995|2.6943377999999996|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 15:00:00|           2.73482|3.7316300000000004|         52.074814|2.7827827999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 16:00:00|2.6398816000000003|          3.834493|         54.961468|         2.8663425|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 17:00:00|         2.5602984|         3.5236087|          57.12959|          2.730953|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 18:00:00|2.3228847999999997|         3.1005373| 63.62944399999999|         2.3298986|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 19:00:00|         2.3371992|         3.0020676| 64.78435999999999|2.1526132000000002|POINT (702000 469...|  Lazio|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lazio\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM10\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/lazio/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de164da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:13:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/20 13:13:15 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/20 13:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/20 13:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/20 13:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/20 13:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/20 13:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry| DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|902000.0|4576000.0|20.0|2021-05-21 00:00:00|2.5389232999999995|         2.9607124|         102.89293|         2.4104354|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 01:00:00|         2.5921166|         2.9856164| 98.94353000000001|2.6902790000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 02:00:00|2.5095107999999997|2.8129014999999997|102.21184000000001|2.9086206000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 03:00:00|         2.4757087|         2.7655613|          102.6552|         3.2857485|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 04:00:00|2.4431244999999997|          2.733494|          99.22969|            2.8583|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 05:00:00|          2.602668|         2.9043045|         91.063896|         3.1135564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 06:00:00|         2.9900506|3.3243476999999997|          86.42286|          4.102874|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 07:00:00|         3.1773403|          3.527025| 92.17261500000001|          4.456797|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 08:00:00|2.9301939999999997|3.2793129999999997|         103.37425|          3.982049|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 09:00:00|2.4819592999999998|         2.7998161|         112.91657|         2.0289207|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 10:00:00|1.9674328999999997|         2.3240533|         117.71718|        0.98944813|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 11:00:00|         1.7692596|         2.1361778|         117.36697|        0.75310564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 12:00:00|1.7476258000000002|         2.1464431|         117.83322|         0.6366417|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 13:00:00|2.1857892999999997|         2.9506361|        119.379616|         0.6609495|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 14:00:00|2.3429406000000004|         3.6170475|         116.32994|0.6980888000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 15:00:00|         2.7623644|          4.243285|119.94583999999999|0.7787763000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 16:00:00|         2.9903412|4.7465269999999995|         122.55632|        0.98343766|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 17:00:00|         3.0250092|         5.0886664|        123.061676|1.0989118999999998|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 18:00:00|         3.0932174|         5.3077917|121.08698999999999|         1.4064169|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 19:00:00|         2.9959621|         5.2067146|         118.31918|         1.4490062|POINT (902000 457...|Campania|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Campania\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM10\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/campania/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e30d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+---------+\n",
      "| original_date_time|            c_PM10|       x|        y|\n",
      "+-------------------+------------------+--------+---------+\n",
      "|2021-05-21 00:00:00|         2.9607124|902000.0|4576000.0|\n",
      "|2021-05-21 01:00:00|         2.9856164|902000.0|4576000.0|\n",
      "|2021-05-21 02:00:00|2.8129014999999997|902000.0|4576000.0|\n",
      "|2021-05-21 03:00:00|         2.7655613|902000.0|4576000.0|\n",
      "|2021-05-21 04:00:00|          2.733494|902000.0|4576000.0|\n",
      "|2021-05-21 05:00:00|         2.9043045|902000.0|4576000.0|\n",
      "|2021-05-21 06:00:00|3.3243476999999997|902000.0|4576000.0|\n",
      "|2021-05-21 07:00:00|          3.527025|902000.0|4576000.0|\n",
      "|2021-05-21 08:00:00|3.2793129999999997|902000.0|4576000.0|\n",
      "|2021-05-21 09:00:00|         2.7998161|902000.0|4576000.0|\n",
      "|2021-05-21 10:00:00|         2.3240533|902000.0|4576000.0|\n",
      "|2021-05-21 11:00:00|         2.1361778|902000.0|4576000.0|\n",
      "|2021-05-21 12:00:00|         2.1464431|902000.0|4576000.0|\n",
      "|2021-05-21 13:00:00|         2.9506361|902000.0|4576000.0|\n",
      "|2021-05-21 14:00:00|         3.6170475|902000.0|4576000.0|\n",
      "|2021-05-21 15:00:00|          4.243285|902000.0|4576000.0|\n",
      "|2021-05-21 16:00:00|4.7465269999999995|902000.0|4576000.0|\n",
      "|2021-05-21 17:00:00|         5.0886664|902000.0|4576000.0|\n",
      "|2021-05-21 18:00:00|         5.3077917|902000.0|4576000.0|\n",
      "|2021-05-21 19:00:00|         5.2067146|902000.0|4576000.0|\n",
      "+-------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = df.withColumnRenamed(\"time\", \"original_date_time\")\n",
    "\n",
    "data_PM10 = data.select(col(\"original_date_time\"),col(\"c_PM10\"), col(\"x\"), col(\"y\"))\n",
    "data_PM10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eeb9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ada82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "train = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train = train.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test = test.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2bd4c",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb209c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  16911.20681285858\n",
      "Time taken for forecasting:  0.16312050819396973\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_dt = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_output_path = \"output/decision_tree_regression_model_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_output_path = \"output/decision_tree_regression_forecast_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_output_path):\n",
    "    os.makedirs(forecast_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train_dt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test_dt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db60af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "# Load the saved decision tree regression model\n",
    "model_dt = DecisionTreeRegressionModel.load(\"output/decision_tree_regression_model_PM10/\")\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e048aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.1688708893378488\n",
      "R2 (Train Data): 0.9459054409797093\n",
      "MSE (Train Data): 4.704000934617153\n",
      "MAE (Train Data): 0.5443879876868724\n"
     ]
    }
   ],
   "source": [
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582a1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.4931748254548303\n",
      "R2 (Prediction Data): 0.9679679865455697\n",
      "MSE (Prediction Data): 2.229571059372063\n",
      "MAE (Prediction Data): 0.476955306001538\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e955a",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f62b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.2666940460859157\n",
      "R2 (Train Data): 0.9555905432782414\n",
      "MSE (Train Data): 10.671289990733172\n",
      "MAE (Train Data): 0.8640960490436875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  611.3809916973114\n",
      "Time taken for forecasting:  0.07816171646118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=====================================================>  (89 + 4) / 93]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.347045897702002\n",
      "R2 (Prediction Data): 0.9870110131177546\n",
      "MSE (Prediction Data): 1.8145326505157915\n",
      "MAE (Prediction Data): 0.5817024731284909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_model_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_forecast_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15665a37",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99884d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.253683799510778\n",
      "R2 (Train Data): 0.8793899406312137\n",
      "MSE (Train Data): 10.586458267198893\n",
      "MAE (Train Data): 0.6442333279760061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  514.2834293842316\n",
      "Time taken for forecasting:  0.07926702499389648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:============================================>          (58 + 13) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.413149705812975\n",
      "R2 (Prediction Data): 0.9558717365043055\n",
      "MSE (Prediction Data): 1.9969920910392978\n",
      "MAE (Prediction Data): 0.42398465598405366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd692e5",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391557da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.326939013047208\n",
      "R2 (Train Data): 0.9338834240983207\n",
      "MSE (Train Data): 5.414645170441114\n",
      "MAE (Train Data): 0.6332111581412158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  673.7216317653656\n",
      "Time taken for forecasting:  0.31071949005126953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.8059912137432244\n",
      "R2 (Prediction Data): 0.9439000909057687\n",
      "MSE (Prediction Data): 3.2616042641177243\n",
      "MAE (Prediction Data): 0.5088276401165229\n"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c00691",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa85fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  16587.942806243896\n",
      "Time taken for forecasting:  0.17026233673095703\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5, numTrees=10)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"output/random_forest_regression_model_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"output/random_forest_regression_forecast_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf35a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.790519541584656\n",
      "R2 (Prediction Data): 0.8881249979981478\n",
      "MSE (Prediction Data): 7.78699931196584\n",
      "MAE (Prediction Data): 1.137580034714237\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a9809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.4208360108226668\n",
      "R2 (Train Data): 0.865429242807\n",
      "MSE (Train Data): 11.702119012941132\n",
      "MAE (Train Data): 1.201535035492181\n",
      "Time taken for training:  51343.010815143585\n",
      "Time taken for forecasting:  0.08656907081604004\n",
      "RMSE (Prediction Data): 2.4721443993163437\n",
      "R2 (Prediction Data): 0.9121967505220815\n",
      "MSE (Prediction Data): 6.111497931071168\n",
      "MAE (Prediction Data): 0.9676060172507076\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7d274",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7d2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 4.55933281049611\n",
      "R2 (Train Data): 0.9134910326112085\n",
      "MSE (Train Data): 20.78751567686636\n",
      "MAE (Train Data): 1.5369512495776558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  898.9193108081818\n",
      "Time taken for forecasting:  0.0850062370300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:====================================================>   (85 + 5) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.6014593597641005\n",
      "R2 (Prediction Data): 0.9515554883472794\n",
      "MSE (Prediction Data): 6.767590800504243\n",
      "MAE (Prediction Data): 1.1833911500633758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 92:=======================================================>(89 + 1) / 90]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759712e4",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193c9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.9661582522171894\n",
      "R2 (Train Data): 0.8997645192918138\n",
      "MSE (Train Data): 8.798094777196132\n",
      "MAE (Train Data): 1.2004618469152044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  352.4724473953247\n",
      "Time taken for forecasting:  0.09742546081542969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:=====================================>                 (48 + 23) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.3542285245334926\n",
      "R2 (Prediction Data): 0.8775277412089411\n",
      "MSE (Prediction Data): 5.542391945727147\n",
      "MAE (Prediction Data): 0.9820306674982977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d096234",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4651c6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.516098381019138\n",
      "R2 (Train Data): 0.8490398257853192\n",
      "MSE (Train Data): 12.362947825005403\n",
      "MAE (Train Data): 1.2398317878079506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  150.0333228111267\n",
      "Time taken for forecasting:  0.30048084259033203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 3.4372699331677192\n",
      "R2 (Prediction Data): 0.7967838732156487\n",
      "MSE (Prediction Data): 11.814824593458816\n",
      "MAE (Prediction Data): 1.0846226100158662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:=======================================>               (53 + 20) / 73]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM10\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train_rf)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522dbaf",
   "metadata": {},
   "source": [
    "# GradientBoosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be32dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.247085856682808\n",
      "R2 (Train Data): 0.9419335175777418\n",
      "MSE (Train Data): 5.0493948473039065\n",
      "MAE (Train Data): 0.4753357435233894\n",
      "Time taken for training:  170636.9153022766\n",
      "Time taken for forecasting:  0.06767821311950684\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-26ff8816392c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Load the saved predictions from the Parquet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mpredictions_gbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_gbt_output_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    299\u001b[0m                        int96RebaseMode=int96RebaseMode)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gporq3/store_0/usr/nafis/Thesis/myenv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM10\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"output/gradient_boosted_regression_model_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"output/gradient_boosted_regression_forecast_PM10/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d780a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.5104695837534492\n",
      "R2 (Prediction Data): 0.9672216650789504\n",
      "MSE (Prediction Data): 2.281518363444318\n",
      "MAE (Prediction Data): 0.38502529431295385\n"
     ]
    }
   ],
   "source": [
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce3ebe",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391c0c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.982999377919127\n",
      "R2 (Train Data): 0.933979468620205\n",
      "MSE (Train Data): 15.864284044504153\n",
      "MAE (Train Data): 0.863054956843091\n",
      "Time taken for training:  630.6854968070984\n",
      "Time taken for forecasting:  0.07400250434875488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 321:==========================================>           (70 + 20) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.7410197108603995\n",
      "R2 (Prediction Data): 0.9783020918263338\n",
      "MSE (Prediction Data): 3.0311496336044303\n",
      "MAE (Prediction Data): 0.495877198390421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM10\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_model_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_forecast_PM10_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef5a39",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea18be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.075512446144151\n",
      "R2 (Train Data): 0.8922374600286069\n",
      "MSE (Train Data): 9.458776806387581\n",
      "MAE (Train Data): 0.5284588934292506\n",
      "Time taken for training:  448.691428899765\n",
      "Time taken for forecasting:  0.06876850128173828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 304:===========================================>          (57 + 14) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3320451652078433\n",
      "R2 (Prediction Data): 0.9607916655597099\n",
      "MSE (Prediction Data): 1.7743443221535906\n",
      "MAE (Prediction Data): 0.30228913890573805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM10\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM10_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108a9a0",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6233c53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:43:37 WARN BlockManager: Asked to remove block rdd_464_85, which does not exist\n",
      "23/06/20 13:43:37 WARN BlockManager: Asked to remove block rdd_464_33, which does not exist\n",
      "23/06/20 13:43:37 WARN BlockManager: Asked to remove block rdd_464_72, which does not exist\n",
      "23/06/20 13:43:37 WARN BlockManager: Asked to remove block rdd_464_47, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:===================>                                 (39 + 69) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:43:37 WARN BlockManager: Asked to remove block rdd_464_58, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_52, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_24, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_23, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_64, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_92, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_11, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_30, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_46, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_5, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_21, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_66, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_69, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_61, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_35, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_59, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:===================>                                 (40 + 68) / 108]\r",
      "\r",
      "[Stage 161:=====================>                               (44 + 64) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_7, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_25, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_13, which does not exist\n",
      "23/06/20 13:43:38 WARN BlockManager: Asked to remove block rdd_464_34, which does not exist\n",
      "23/06/20 13:43:39 WARN BlockManager: Asked to remove block rdd_464_1, which does not exist\n",
      "23/06/20 13:43:39 WARN BlockManager: Asked to remove block rdd_464_27, which does not exist\n",
      "23/06/20 13:43:39 WARN BlockManager: Asked to remove block rdd_464_67, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:======================================>              (79 + 29) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:43:39 WARN BlockManager: Asked to remove block rdd_464_28, which does not exist\n",
      "23/06/20 13:43:39 WARN BlockManager: Asked to remove block rdd_464_19, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:==================>                                  (38 + 70) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_57, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_5, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_32, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_0, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_92, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_45, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_30, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_20, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_15, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_62, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_61, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_23, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_7, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_19, which does not exist\n",
      "23/06/20 13:44:23 WARN BlockManager: Asked to remove block rdd_574_54, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_65, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_52, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 211:===================>                                 (39 + 69) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_6, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_13, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_26, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_41, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_9, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_25, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_43, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_39, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_12, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_33, which does not exist\n",
      "23/06/20 13:44:24 WARN BlockManager: Asked to remove block rdd_574_58, which does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.6411579695947474\n",
      "R2 (Train Data): 0.9148216727891763\n",
      "MSE (Train Data): 6.975715420353848\n",
      "MAE (Train Data): 0.5959209967058723\n",
      "Time taken for training:  259.26708126068115\n",
      "Time taken for forecasting:  0.08152413368225098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.1721319709914675\n",
      "R2 (Prediction Data): 0.9188472376919685\n",
      "MSE (Prediction Data): 4.718157299403277\n",
      "MAE (Prediction Data): 0.4207901784905772\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM10.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM10.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM10\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM10', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM10_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM10\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9967e8",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a20de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "|original_date_time|   c_PM10|    x|     y|            features|   prediction_gbt|\n",
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "|       1.6375356E9| 8.895134|266.0|4016.0|[1.6375356E9,266....|8.797996762245894|\n",
      "|       1.6375392E9| 8.681462|266.0|4016.0|[1.6375392E9,266....|8.797996762245894|\n",
      "|       1.6375428E9| 8.231853|266.0|4016.0|[1.6375428E9,266....|8.570813672875126|\n",
      "|       1.6375464E9|7.9760885|266.0|4016.0|[1.6375464E9,266....|7.927370096995555|\n",
      "|         1.63755E9|7.8632627|266.0|4016.0|[1.63755E9,266.0,...|7.927370096995555|\n",
      "+------------------+---------+-----+------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+------+------------------+\n",
      "|original_date_time|    c_PM10|    x|     y|        prediction|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "|       1.6250904E9| 5.2231255|270.0|4428.0| 5.209145992769982|\n",
      "|       1.6250904E9|0.88116604|270.0|5016.0| 1.935830543140013|\n",
      "|       1.6250904E9| 10.994879|274.0|4032.0|10.807470633496024|\n",
      "|       1.6250904E9| 15.053389|282.0|4768.0|14.492677321055895|\n",
      "|       1.6250904E9| 0.9017975|282.0|5012.0| 1.935830543140013|\n",
      "+------------------+----------+-----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:=====================================================>(499 + 1) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.5798773405914597\n",
      "R2 (Prediction Data): 0.964140051195009\n",
      "MSE (Prediction Data): 2.4960124113143416\n",
      "MAE (Prediction Data): 0.4425090309509558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens = (\n",
    "    spark.read.parquet(\"output/decision_tree_regression_forecast_PM10/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM10/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens = (\n",
    "    spark.read.parquet(\"output/gradient_boosted_regression_forecast_PM10/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions = (\n",
    "    predictions_dt_ens\n",
    "    .join(predictions_rf_ens, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .join(predictions_gbt_ens, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM10\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions.show(5)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238975b6",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbbece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|            c_PM10|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6390908E9|         30.808115|462000.0|5016000.0|[1.6390908E9,4620...|32.189725283064696|\n",
      "|       1.6390944E9|26.799584999999997|462000.0|5016000.0|[1.6390944E9,4620...| 28.39424642581758|\n",
      "|        1.639098E9|          24.16349|462000.0|5016000.0|[1.639098E9,46200...|25.216014785485985|\n",
      "|       1.6391016E9|22.022879999999997|462000.0|5016000.0|[1.6391016E9,4620...| 22.52873511007672|\n",
      "|       1.6391052E9|         20.484999|462000.0|5016000.0|[1.6391052E9,4620...|20.216560578339365|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|            c_PM10|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|           7.46137|478000.0|4988000.0| 7.255583827203072|\n",
      "|       1.6250904E9|0.9632886999999999|478000.0|5072000.0|1.9261752017114722|\n",
      "|       1.6250904E9| 6.919819400000001|482000.0|4996000.0| 6.911549057073851|\n",
      "|       1.6250904E9|          8.044309|490000.0|5000000.0| 8.217194606334488|\n",
      "|       1.6250904E9|          6.809235|490000.0|5004000.0| 6.911549057073851|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:====================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.4604561331082082\n",
      "R2 (Prediction Data): 0.9847318110935647\n",
      "MSE (Prediction Data): 2.13293211673338\n",
      "MAE (Prediction Data): 0.49610556999964955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM10_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM10_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM10_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lombardia = (\n",
    "    predictions_dt_ens_lombardia\n",
    "    .join(predictions_rf_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .join(predictions_gbt_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM10\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lombardia.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lambordia = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lambordia}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lambordia}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lambordia}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lambordia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ef84a",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45151ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|   c_PM10|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "|       1.6295832E9| 4.448652|702000.0|4696000.0|[1.6295832E9,7020...| 4.704100072583649|\n",
      "|       1.6295868E9|4.9819837|702000.0|4696000.0|[1.6295868E9,7020...| 4.900844449390115|\n",
      "|       1.6295904E9|  5.33442|702000.0|4696000.0|[1.6295904E9,7020...|5.1807169155050925|\n",
      "|        1.629594E9| 5.532685|702000.0|4696000.0|[1.629594E9,70200...| 5.735783045089824|\n",
      "|       1.6295976E9|5.5827894|702000.0|4696000.0|[1.6295976E9,7020...| 5.735783045089824|\n",
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------+---------+------------------+\n",
      "|original_date_time|   c_PM10|       x|        y|        prediction|\n",
      "+------------------+---------+--------+---------+------------------+\n",
      "|       1.6250904E9| 6.302264|714000.0|4688000.0| 6.293649837562079|\n",
      "|       1.6250904E9| 9.482507|746000.0|4684000.0| 9.574086383597626|\n",
      "|       1.6250904E9|11.656464|754000.0|4676000.0|11.443720687792469|\n",
      "|       1.6250904E9|10.149933|754000.0|4684000.0|  9.73207861550226|\n",
      "|       1.6250904E9|13.774732|770000.0|4664000.0|13.511801872055415|\n",
      "+------------------+---------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 347:===========================>                         (52 + 48) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3039292407569298\n",
      "R2 (Prediction Data): 0.9624293449979241\n",
      "MSE (Prediction Data): 1.7002314649009438\n",
      "MAE (Prediction Data): 0.37285734519254093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 347:==================================================>   (94 + 6) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM10_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM10_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM10_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lazio.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lazio = (\n",
    "    predictions_dt_ens_lazio\n",
    "    .join(predictions_rf_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .join(predictions_gbt_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM10\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lazio.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lazio = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lazio}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lazio}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lazio}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lazio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca62512",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef03c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|   c_PM10|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "|       1.6381404E9| 16.29053|902000.0|4576000.0|[1.6381404E9,9020...|16.290915851947233|\n",
      "|        1.638144E9|16.214872|902000.0|4576000.0|[1.638144E9,90200...|16.290915851947233|\n",
      "|       1.6381476E9|  15.1627|902000.0|4576000.0|[1.6381476E9,9020...|14.803951376668849|\n",
      "|       1.6381512E9|14.572713|902000.0|4576000.0|[1.6381512E9,9020...|14.803951376668849|\n",
      "|       1.6381548E9|10.255486|902000.0|4576000.0|[1.6381548E9,9020...|10.023107910381817|\n",
      "+------------------+---------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|            c_PM10|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9| 5.725661799999999|910000.0|4572000.0| 5.655687413630916|\n",
      "|       1.6250904E9|3.5256982000000003|914000.0|4600000.0|3.5835783263304397|\n",
      "|       1.6250904E9| 9.373660000000001|926000.0|4528000.0| 9.176784080735118|\n",
      "|       1.6250904E9|          9.137948|930000.0|4560000.0| 8.889156224257249|\n",
      "|       1.6250904E9|          7.446572|942000.0|4564000.0| 7.436276381897188|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 351:====>                                                 (9 + 91) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.1460169270576386\n",
      "R2 (Prediction Data): 0.920786869357734\n",
      "MSE (Prediction Data): 4.60538865121791\n",
      "MAE (Prediction Data): 0.487667838276827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 351:=================================================>    (91 + 9) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM10_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM10_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM10_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_campania.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_campania = (\n",
    "    predictions_dt_ens_campania\n",
    "    .join(predictions_rf_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .join(predictions_gbt_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_PM10\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM10\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_campania.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_campania = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM10\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_campania}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_campania}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_campania}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_campania}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891aa487",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918fa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
