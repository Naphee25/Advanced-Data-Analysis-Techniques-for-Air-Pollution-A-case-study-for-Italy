{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bae62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 18:58:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 18:58:51 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/05/07 18:58:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM25: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM25\").config(\"spark.driver.memory\", \"16g\").config(\"spark.executor.memory\", \"8g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.driver.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.executor.localDir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"../dataset/csv/\", header=True, schema=schema)\n",
    "\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2')\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b27290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|  DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "|462000.0|5016000.0|20.0|2019-11-24 00:00:00|         1.6957362|         2.7321966|          50.69704| 4.208417400000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 01:00:00|1.5195319999999999|         2.5212698|         52.217495|3.6308307999999996|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 02:00:00|1.4698639999999998|          2.540143|53.528316000000004|         3.0628326|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 03:00:00|         1.4075043|         2.5400522|         55.269703|          2.444656|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 04:00:00|1.3243808000000001|         2.4868004| 56.09563000000001|         1.9309062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 05:00:00|1.2917066000000001|         2.4511726|56.588367000000005|         1.6992023|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 06:00:00|         1.3729354|         2.5612066|          56.55035|1.5480975000000001|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 07:00:00|         1.8005681|         2.9130769|          53.39305|         2.1196866|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 08:00:00|         2.7273355|         3.6733246|         45.281013|         4.0332394|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 09:00:00|4.3827419999999995|         5.3303866|         39.011024|         7.3524475|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 10:00:00| 4.251999400000001|         5.3588486|         42.592384|          6.889144|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 11:00:00|3.8547763999999995|          4.963749|42.843109999999996|          6.416884|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 12:00:00|         3.8144705|         4.8981695|         41.092995|6.6946254000000005|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 13:00:00|         3.4577906| 4.584929499999999|42.440692999999996|          6.609909|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 14:00:00|          6.724634|           8.32361|         40.356117|         11.530877|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 15:00:00|         6.9248476| 8.738185000000001|         39.104855|         15.190662|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 16:00:00|         4.1298723|         5.5023584|          37.11807|         13.312062|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 17:00:00|          2.274489|            3.0258|         39.018482|         7.7159667|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 18:00:00|         2.2451131|3.0147939999999998|         35.107414|          8.846022|POINT (462000 501...|Lombardia|\n",
      "|462000.0|5016000.0|20.0|2019-11-24 19:00:00|         2.5813787|          3.473392|          33.05836|          9.195902|POINT (462000 501...|Lombardia|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lombardia\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM25\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/data_chunk/lombardia/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc6ab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/19 10:07:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/19 10:07:21 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/19 10:07:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/19 10:07:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry|DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "|702000.0|4696000.0|20.0|2021-12-11 00:00:00|         1.2706414|1.5203451000000001|60.837047999999996|         2.0367115|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 01:00:00|          1.230251|          1.375211|         57.657654|         2.0870926|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 02:00:00|1.9736630000000002|          2.107126|          54.05386|          2.587756|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 03:00:00|1.7445994999999999|1.8961723000000001|          54.00688|         2.1309106|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 04:00:00|         1.2854077|1.5713663999999998|54.418915000000005|1.6039573999999999|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 05:00:00|0.8523575999999999|         1.1182661| 53.96324499999999|         1.8012519|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 06:00:00|        0.78199506|        0.94059986|         48.937576|          2.225234|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 07:00:00|         1.2314979|         1.4721483|46.245734999999996|         2.2615266|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 08:00:00|         2.3940308|         3.0101209|          51.23545|2.1766273999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 09:00:00|         3.0954514|3.8334242999999995|         51.540134|2.1541189999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 10:00:00|         2.8416717|3.5963568999999995|           54.0782|1.7994093999999998|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 11:00:00|3.1728162999999996|          3.890354|          54.80785|1.9321240000000002|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 12:00:00|         2.9845037|         3.5793805|52.454944999999995|         2.0442376|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 13:00:00|3.3172165999999996|         4.0856614|         52.290283|         2.3790326|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 14:00:00|         3.1035097|         3.9299242|52.074946999999995|2.6943377999999996|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 15:00:00|           2.73482|3.7316300000000004|         52.074814|2.7827827999999997|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 16:00:00|2.6398816000000003|          3.834493|         54.961468|         2.8663425|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 17:00:00|         2.5602984|         3.5236087|          57.12959|          2.730953|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 18:00:00|2.3228847999999997|         3.1005373| 63.62944399999999|         2.3298986|POINT (702000 469...|  Lazio|\n",
      "|702000.0|4696000.0|20.0|2021-12-11 19:00:00|         2.3371992|         3.0020676| 64.78435999999999|2.1526132000000002|POINT (702000 469...|  Lazio|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Lazio\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM25\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/lazio/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a054d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 14:11:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/20 14:11:41 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/20 14:11:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/20 14:11:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|       x|        y|   z|               time|           c_PM2_5|            c_PM10|              c_O3|             c_NO2|            geometry| DEN_REG|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "|902000.0|4576000.0|20.0|2021-05-21 00:00:00|2.5389232999999995|         2.9607124|         102.89293|         2.4104354|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 01:00:00|         2.5921166|         2.9856164| 98.94353000000001|2.6902790000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 02:00:00|2.5095107999999997|2.8129014999999997|102.21184000000001|2.9086206000000003|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 03:00:00|         2.4757087|         2.7655613|          102.6552|         3.2857485|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 04:00:00|2.4431244999999997|          2.733494|          99.22969|            2.8583|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 05:00:00|          2.602668|         2.9043045|         91.063896|         3.1135564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 06:00:00|         2.9900506|3.3243476999999997|          86.42286|          4.102874|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 07:00:00|         3.1773403|          3.527025| 92.17261500000001|          4.456797|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 08:00:00|2.9301939999999997|3.2793129999999997|         103.37425|          3.982049|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 09:00:00|2.4819592999999998|         2.7998161|         112.91657|         2.0289207|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 10:00:00|1.9674328999999997|         2.3240533|         117.71718|        0.98944813|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 11:00:00|         1.7692596|         2.1361778|         117.36697|        0.75310564|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 12:00:00|1.7476258000000002|         2.1464431|         117.83322|         0.6366417|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 13:00:00|2.1857892999999997|         2.9506361|        119.379616|         0.6609495|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 14:00:00|2.3429406000000004|         3.6170475|         116.32994|0.6980888000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 15:00:00|         2.7623644|          4.243285|119.94583999999999|0.7787763000000001|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 16:00:00|         2.9903412|4.7465269999999995|         122.55632|        0.98343766|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 17:00:00|         3.0250092|         5.0886664|        123.061676|1.0989118999999998|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 18:00:00|         3.0932174|         5.3077917|121.08698999999999|         1.4064169|POINT (902000 457...|Campania|\n",
      "|902000.0|4576000.0|20.0|2021-05-21 19:00:00|         2.9959621|         5.2067146|         118.31918|         1.4490062|POINT (902000 457...|Campania|\n",
      "+--------+---------+----+-------------------+------------------+------------------+------------------+------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- c_PM2_5: double (nullable = true)\n",
      " |-- c_PM10: double (nullable = true)\n",
      " |-- c_O3: double (nullable = true)\n",
      " |-- c_NO2: double (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      " |-- DEN_REG: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Campania\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyproj\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PM25\").config('spark.sql.shuffle.partitions',500).config('spark.driver.maxResultSize', '10G') .config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.task.maxFailures\", \"10\").config(\"spark.executor.instances\", \"16\").config(\"spark.local.dir\", \"/afs/enea.it/por/user/nafis/PFS/tmp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "\n",
    "schema = StructType(\n",
    "   [StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('c_PM25', DoubleType(), True),\n",
    "    StructField('c_PM10', DoubleType(), True),\n",
    "    StructField('c_O3', DoubleType(), True),\n",
    "    StructField('c_NO2', DoubleType(), True),\n",
    "    StructField('geometry', StringType(), True),\n",
    "    StructField('index_right', IntegerType(), True),\n",
    "    StructField('COD_RIP', IntegerType(), True),\n",
    "    StructField('COD_REG', IntegerType(), True),\n",
    "    StructField('DEN_REG', StringType(), True),\n",
    "    StructField('Shape_Leng', DoubleType(), True),\n",
    "    StructField('Shape_Area', DoubleType(), True),\n",
    "   ]\n",
    "  )\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/data_chunk/campania/\", header=True, schema=schema)\n",
    "\n",
    "# Select the columns in the desired order\n",
    "df = df.select( 'x', 'y', 'z', 'time', 'c_PM25', 'c_PM10', 'c_O3', 'c_NO2', 'geometry', 'DEN_REG')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"x\", df[\"x\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"y\", df[\"y\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"z\", df[\"z\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"time\", df[\"time\"].cast(TimestampType())) \\\n",
    "    .withColumn(\"c_PM25\", df[\"c_PM25\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_PM10\", df[\"c_PM10\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_O3\", df[\"c_O3\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"c_NO2\", df[\"c_NO2\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"geometry\", df[\"geometry\"].cast(StringType()))\\\n",
    "    .withColumn(\"DEN_REG\", df[\"DEN_REG\"].cast(StringType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"c_PM25\", \"c_PM2_5\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76e67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+---------+\n",
      "| original_date_time|           c_PM2_5|       x|        y|\n",
      "+-------------------+------------------+--------+---------+\n",
      "|2021-05-21 00:00:00|2.5389232999999995|902000.0|4576000.0|\n",
      "|2021-05-21 01:00:00|         2.5921166|902000.0|4576000.0|\n",
      "|2021-05-21 02:00:00|2.5095107999999997|902000.0|4576000.0|\n",
      "|2021-05-21 03:00:00|         2.4757087|902000.0|4576000.0|\n",
      "|2021-05-21 04:00:00|2.4431244999999997|902000.0|4576000.0|\n",
      "|2021-05-21 05:00:00|          2.602668|902000.0|4576000.0|\n",
      "|2021-05-21 06:00:00|         2.9900506|902000.0|4576000.0|\n",
      "|2021-05-21 07:00:00|         3.1773403|902000.0|4576000.0|\n",
      "|2021-05-21 08:00:00|2.9301939999999997|902000.0|4576000.0|\n",
      "|2021-05-21 09:00:00|2.4819592999999998|902000.0|4576000.0|\n",
      "|2021-05-21 10:00:00|1.9674328999999997|902000.0|4576000.0|\n",
      "|2021-05-21 11:00:00|         1.7692596|902000.0|4576000.0|\n",
      "|2021-05-21 12:00:00|1.7476258000000002|902000.0|4576000.0|\n",
      "|2021-05-21 13:00:00|2.1857892999999997|902000.0|4576000.0|\n",
      "|2021-05-21 14:00:00|2.3429406000000004|902000.0|4576000.0|\n",
      "|2021-05-21 15:00:00|         2.7623644|902000.0|4576000.0|\n",
      "|2021-05-21 16:00:00|         2.9903412|902000.0|4576000.0|\n",
      "|2021-05-21 17:00:00|         3.0250092|902000.0|4576000.0|\n",
      "|2021-05-21 18:00:00|         3.0932174|902000.0|4576000.0|\n",
      "|2021-05-21 19:00:00|         2.9959621|902000.0|4576000.0|\n",
      "+-------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.withColumnRenamed(\"time\", \"original_date_time\")\n",
    "\n",
    "data_PM25 = data.select(col(\"original_date_time\"),col(\"c_PM2_5\"), col(\"x\"), col(\"y\"))\n",
    "data_PM25.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291b42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99092e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "train = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train = train.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test = test.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM2_5\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74627a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18913548"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b60a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7583652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56534e02",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c30ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  16063.485694169998\n",
      "Time taken for forecasting:  0.1763441562652588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_dt = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_dt = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_dt = train_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_dt = test_dt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_dt = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM2_5\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_dt = assembler_dt.transform(train_dt)\n",
    "test_dt = assembler_dt.transform(test_dt)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_output_path = \"output/decision_tree_regression_model_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_output_path = \"output/decision_tree_regression_forecast_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_output_path):\n",
    "    os.makedirs(forecast_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train_dt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test_dt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbce7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "# Load the saved decision tree regression model\n",
    "model_dt = DecisionTreeRegressionModel.load(\"output/decision_tree_regression_model_PM2_5/\")\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b36d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 1.579756501786754\n",
      "R2 (Train Data): 0.9384725575786191\n",
      "MSE (Train Data): 2.4956306049375216\n",
      "MAE (Train Data): 0.4241766044094081\n"
     ]
    }
   ],
   "source": [
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fe583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.1953295256646943\n",
      "R2 (Prediction Data): 0.9457080666860506\n",
      "MSE (Prediction Data): 1.4288126749257832\n",
      "MAE (Prediction Data): 0.3204468954629573\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d4c6a",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9bbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 4.104373392309294\n",
      "R2 (Train Data): 0.9210679518881306\n",
      "MSE (Train Data): 16.84588094349651\n",
      "MAE (Train Data): 0.950471267920366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  42.721378803253174\n",
      "Time taken for forecasting:  0.0630192756652832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:======================================================> (88 + 2) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.7569088789617495\n",
      "R2 (Prediction Data): 0.9754749669583337\n",
      "MSE (Prediction Data): 3.086728808974631\n",
      "MAE (Prediction Data): 0.5950311849676214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_model_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/decision_tree_regression_forecast_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a98fdb",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b04e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.484062925084526\n",
      "R2 (Train Data): 0.8185710297254536\n",
      "MSE (Train Data): 12.138694465948543\n",
      "MAE (Train Data): 0.6169562713998137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  405.41633892059326\n",
      "Time taken for forecasting:  0.09121894836425781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:========================================>              (52 + 19) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.6239749597175892\n",
      "R2 (Prediction Data): 0.9126985149180113\n",
      "MSE (Prediction Data): 2.637294669789744\n",
      "MAE (Prediction Data): 0.35226079561529766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:==================================================>     (64 + 7) / 71]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a007fef",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8e1f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.225030727885437\n",
      "R2 (Train Data): 0.912517125804106\n",
      "MSE (Train Data): 4.950761740034398\n",
      "MAE (Train Data): 0.48189363965038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  87.42386412620544\n",
      "Time taken for forecasting:  0.08373260498046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=======================================================>(71 + 1) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.9608848171192985\n",
      "R2 (Prediction Data): 0.9057409184182872\n",
      "MSE (Prediction Data): 3.8450692660089847\n",
      "MAE (Prediction Data): 0.37791542064818007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Regression model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_model_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_dt_output_path):\n",
    "    os.makedirs(model_dt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_dt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_dt_output_path):\n",
    "    os.makedirs(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_dt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_dt.write().overwrite().save(model_dt_output_path)\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_dt = model_dt.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"rmse\"})\n",
    "r2_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"r2\"})\n",
    "mae_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mae\"})\n",
    "mse_dt_train = evaluator.evaluate(predictions_train_dt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_dt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_dt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_dt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_dt_train}\")\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_dt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_dt = model_dt.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_dt = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_dt.write.mode(\"overwrite\").parquet(forecast_dt_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_dt = training_end_time_dt - training_start_time_dt\n",
    "print(\"Time taken for training: \", training_time_dt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_dt = forecast_end_time_dt - forecast_start_time_dt\n",
    "print(\"Time taken for forecasting: \", forecast_time_dt)\n",
    "\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_dt = spark.read.parquet(forecast_dt_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_dt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_dt_test = evaluator_dt.evaluate(predictions_dt, {evaluator_dt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_dt_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_dt_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_dt_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_dt_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8622f7",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a01299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  16598.03115916252\n",
      "Time taken for forecasting:  0.05803322792053223\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the dataframe\n",
    "train_rf = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_rf = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_rf = train_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_rf = test_rf.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_rf = VectorAssembler(inputCols=[\"original_date_time\", \"c_PM2_5\", \"x\", \"y\"], outputCol=\"features\")\n",
    "train_rf = assembler_rf.transform(train_rf)\n",
    "test_rf = assembler_rf.transform(test_rf)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5, numTrees=10)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"output/random_forest_regression_model_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"output/random_forest_regression_forecast_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train_rf)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test_rf)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64aeac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.8791842973232646\n",
      "R2 (Prediction Data): 0.8658166091675981\n",
      "MSE (Prediction Data): 3.5313336233063315\n",
      "MAE (Prediction Data): 0.8737615381134364\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936d6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 1.7451877482029674\n",
      "R2 (Train Data): 0.9249115965022351\n",
      "MSE (Train Data): 3.0456802764777433\n",
      "MAE (Train Data): 0.8007551210692181\n",
      "Time taken for training:  53020.54016709328\n",
      "Time taken for forecasting:  0.08871793746948242\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rmse_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4cfbf5e2ad0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Print the results with metric names including \"Prediction Data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"RMSE (Prediction Data): {rmse_rf}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"R2 (Prediction Data): {r2_rf}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"MSE (Prediction Data): {mse_rf}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rmse_rf' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168d730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.375679891663427\n",
      "R2 (Prediction Data): 0.928089088890567\n",
      "MSE (Prediction Data): 1.8924951643270973\n",
      "MAE (Prediction Data): 0.6485747354766993\n"
     ]
    }
   ],
   "source": [
    "#Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fe0d5",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75605714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 4.154095862913804\n",
      "R2 (Train Data): 0.9191439216156252\n",
      "MSE (Train Data): 17.256512438277582\n",
      "MAE (Train Data): 1.656140783836397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  594.769184589386\n",
      "Time taken for forecasting:  0.08081960678100586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 131:=====================================================> (87 + 3) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.369187273163974\n",
      "R2 (Prediction Data): 0.9554025622568462\n",
      "MSE (Prediction Data): 5.613048335322149\n",
      "MAE (Prediction Data): 1.184421339037392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_model_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110732d8",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1678be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.0870412257449322\n",
      "R2 (Train Data): 0.857564083625617\n",
      "MSE (Train Data): 9.529823529448773\n",
      "MAE (Train Data): 1.235671157971072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  246.0521638393402\n",
      "Time taken for forecasting:  0.0782470703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=======================================>               (51 + 20) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.3084129098695434\n",
      "R2 (Prediction Data): 0.8236035001429058\n",
      "MSE (Prediction Data): 5.328770162452373\n",
      "MAE (Prediction Data): 0.8525831165106678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c7a04",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431bb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 3.0609624128925077\n",
      "R2 (Train Data): 0.8344355805984497\n",
      "MSE (Train Data): 9.369490893140723\n",
      "MAE (Train Data): 0.9995521299091124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training:  435.608029127121\n",
      "Time taken for forecasting:  0.15989041328430176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.761645736527433\n",
      "R2 (Prediction Data): 0.8130373008114897\n",
      "MSE (Prediction Data): 7.626687174080148\n",
      "MAE (Prediction Data): 0.8474343328257676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 81:====================================================>   (69 + 4) / 73]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_model_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_rf_output_path):\n",
    "    os.makedirs(model_rf_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_rf_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_rf_output_path):\n",
    "    os.makedirs(forecast_rf_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_rf = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_rf = rf.fit(train)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_rf = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_rf.write().overwrite().save(model_rf_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_rf = model_rf.transform(train)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"rmse\"})\n",
    "r2_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"r2\"})\n",
    "mae_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mae\"})\n",
    "mse_rf_train = evaluator.evaluate(predictions_train_rf, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# print the results\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_rf_train}\")\n",
    "print(f\"R2 (Train Data): {r2_rf_train}\")\n",
    "print(f\"MSE (Train Data): {mse_rf_train}\")\n",
    "print(f\"MAE (Train Data): {mae_rf_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_rf = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_rf = time.time()\n",
    "\n",
    "# write the forecast to your personal directory\n",
    "predictions_rf.write.mode(\"overwrite\").parquet(forecast_rf_output_path)\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_rf = training_end_time_rf - training_start_time_rf\n",
    "print(\"Time taken for training: \", training_time_rf)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_rf = forecast_end_time_rf - forecast_start_time_rf\n",
    "print(\"Time taken for forecasting: \", forecast_time_rf)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_rf = spark.read.parquet(forecast_rf_output_path)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_rf = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_rf_test = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_rf_test}\")\n",
    "print(f\"R2 (Prediction Data): {r2_rf_test}\")\n",
    "print(f\"MSE (Prediction Data): {mse_rf_test}\")\n",
    "print(f\"MAE (Prediction Data): {mae_rf_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d0871",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0baf93f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 1.8795022641155679\n",
      "R2 (Train Data): 0.9129088016860606\n",
      "MSE (Train Data): 3.532528760815546\n",
      "MAE (Train Data): 0.39689611936408903\n",
      "Time taken for training:  170858.2653915882\n",
      "Time taken for forecasting:  0.0811622142791748\n",
      "RMSE (Prediction Data): 1.3689715569614558\n",
      "R2 (Prediction Data): 0.9287887084387904\n",
      "MSE (Prediction Data): 1.8740831237694722\n",
      "MAE (Prediction Data): 0.26882700271977766\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"output/gradient_boosted_regression_model_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"output/gradient_boosted_regression_forecast_PM2_5/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "# write the predictions to a Parquet file\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e92947",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7452e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.619836349014904\n",
      "R2 (Train Data): 0.9678405974552927\n",
      "MSE (Train Data): 6.863542495619741\n",
      "MAE (Train Data): 0.7015082257323648\n",
      "Time taken for training:  268.6059672832489\n",
      "Time taken for forecasting:  0.0696861743927002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 589:===============================>                      (53 + 37) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3280798894211043\n",
      "R2 (Prediction Data): 0.9859860834620167\n",
      "MSE (Prediction Data): 1.7637961926847736\n",
      "MAE (Prediction Data): 0.4521599083789563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_model_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/gradient_boosted_regression_forecast_PM2_5_lombardia/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "# write the predictions to a Parquet file\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324307a4",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd39a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.7342242273076223\n",
      "R2 (Train Data): 0.8882614812844926\n",
      "MSE (Train Data): 7.475982125195965\n",
      "MAE (Train Data): 0.45500717181553746\n",
      "Time taken for training:  520.8321993350983\n",
      "Time taken for forecasting:  0.08784055709838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 310:===========================================>          (57 + 14) / 71]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.225698736012148\n",
      "R2 (Prediction Data): 0.9502686268352143\n",
      "MSE (Prediction Data): 1.5023373914617775\n",
      "MAE (Prediction Data): 0.24872240748993493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM2_5_lazio/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "# write the predictions to a Parquet file\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213184d",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2633f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Data): 2.314559849692319\n",
      "R2 (Train Data): 0.905335346957194\n",
      "MSE (Train Data): 5.357187297807729\n",
      "MAE (Train Data): 0.4263440603169816\n",
      "Time taken for training:  145.261323928833\n",
      "Time taken for forecasting:  0.07826852798461914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.2419016327761163\n",
      "R2 (Prediction Data): 0.8767882452508445\n",
      "MSE (Prediction Data): 5.026122931044217\n",
      "MAE (Prediction Data): 0.31506644739244255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the dataframe\n",
    "train_gbt = data_PM25.filter(col(\"original_date_time\") <= \"2021-06-30 23:00:00\")\n",
    "test_gbt = data_PM25.filter(col(\"original_date_time\") > \"2021-06-30 23:00:00\")\n",
    "\n",
    "# Convert timestamp column to UNIX timestamp\n",
    "train_gbt = train_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "test_gbt = test_gbt.withColumn(\"original_date_time\", unix_timestamp(col(\"original_date_time\")).cast(\"double\"))\n",
    "\n",
    "# Prepare the input data for modeling\n",
    "assembler_gbt = VectorAssembler(inputCols=[\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"], outputCol=\"features\")\n",
    "train_gbt = assembler_gbt.transform(train_gbt)\n",
    "test_gbt = assembler_gbt.transform(test_gbt)\n",
    "\n",
    "# Create a Gradient Boosted Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='c_PM2_5', maxDepth=5)\n",
    "\n",
    "# specify the output path for the model\n",
    "model_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_model_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(model_gbt_output_path):\n",
    "    os.makedirs(model_gbt_output_path)\n",
    "\n",
    "# specify the output path for the forecast\n",
    "forecast_gbt_output_path = \"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM2_5_campania/\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(forecast_gbt_output_path):\n",
    "    os.makedirs(forecast_gbt_output_path)\n",
    "\n",
    "# Start the timer\n",
    "training_start_time_gbt = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_gbt = gbt.fit(train_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "training_end_time_gbt = time.time()\n",
    "\n",
    "# write the model to your personal directory\n",
    "model_gbt.write().overwrite().save(model_gbt_output_path)\n",
    "\n",
    "# make predictions on the training data\n",
    "predictions_train_gbt = model_gbt.transform(train_gbt)\n",
    "\n",
    "# evaluate the performance of the model on the training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"c_PM2_5\", predictionCol=\"prediction\")\n",
    "rmse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"rmse\"})\n",
    "r2_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"r2\"})\n",
    "mae_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mae\"})\n",
    "mse_gbt_train = evaluator.evaluate(predictions_train_gbt, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "\n",
    "# Print the results with metric names including \"Train Data\"\n",
    "print(f\"RMSE (Train Data): {rmse_gbt_train}\")\n",
    "print(f\"R2 (Train Data): {r2_gbt_train}\")\n",
    "print(f\"MSE (Train Data): {mse_gbt_train}\")\n",
    "print(f\"MAE (Train Data): {mae_gbt_train}\")\n",
    "\n",
    "# Start the timer\n",
    "forecast_start_time_gbt = time.time()\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "# Stop the timer\n",
    "forecast_end_time_gbt = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time_gbt = training_end_time_gbt - training_start_time_gbt\n",
    "print(\"Time taken for training: \", training_time_gbt)\n",
    "\n",
    "# Calculate the time taken for forecasting\n",
    "forecast_time_gbt = forecast_end_time_gbt - forecast_start_time_gbt\n",
    "print(\"Time taken for forecasting: \", forecast_time_gbt)\n",
    "\n",
    "# write the predictions to a Parquet file\n",
    "predictions_gbt.write.mode(\"overwrite\").parquet(forecast_gbt_output_path)\n",
    "\n",
    "# Load the saved predictions from the Parquet file\n",
    "predictions_gbt = spark.read.parquet(forecast_gbt_output_path)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_gbt = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_gbt = evaluator_gbt.evaluate(predictions_gbt, {evaluator_gbt.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_gbt}\")\n",
    "print(f\"R2 (Prediction Data): {r2_gbt}\")\n",
    "print(f\"MSE (Prediction Data): {mse_gbt}\")\n",
    "print(f\"MAE (Prediction Data): {mae_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c3d1c",
   "metadata": {},
   "source": [
    "# Ensemble Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc26854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "|original_date_time|  c_PM2_5|    x|     y|            features|    prediction_gbt|\n",
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "|       1.6446204E9| 4.025283|266.0|4016.0|[1.6446204E9,266....|3.9895026955953923|\n",
      "|        1.644624E9|4.2530026|266.0|4016.0|[1.644624E9,266.0...| 4.427801237154164|\n",
      "|       1.6446276E9|4.7990746|266.0|4016.0|[1.6446276E9,266....|4.5823992290612185|\n",
      "|       1.6446312E9| 5.260711|266.0|4016.0|[1.6446312E9,266....| 5.195336611671761|\n",
      "|       1.6446348E9| 5.463522|266.0|4016.0|[1.6446348E9,266....| 5.195336611671761|\n",
      "+------------------+---------+-----+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+------+------------------+\n",
      "|original_date_time|  c_PM2_5|    x|     y|        prediction|\n",
      "+------------------+---------+-----+------+------------------+\n",
      "|       1.6250904E9|1.8357738|266.0|4392.0| 1.938366765559559|\n",
      "|       1.6250904E9|0.7923123|270.0|5024.0|  1.40429464357588|\n",
      "|       1.6250904E9|1.8936615|274.0|4300.0|2.1362267899644922|\n",
      "|       1.6250904E9|0.7861382|274.0|4980.0|1.4066969016487454|\n",
      "|       1.6250904E9|0.7695013|278.0|4960.0|1.4119811359678487|\n",
      "+------------------+---------+-----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>(0 + 96) / 151][Stage 22:> (0 + 0) / 151][Stage 23:> (0 + 0) / 155]1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 09:48:29 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:239)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:230)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@cresco6-nvi1.portici.enea.it:26833\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/05/08 09:48:29 WARN BlockManagerMasterEndpoint: Error trying to remove shuffle 6. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:26833 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:26833 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 09:48:50 WARN NettyRpcEnv: Ignored message: true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>(0 + 96) / 151][Stage 30:> (0 + 0) / 151][Stage 31:> (0 + 0) / 155]1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 11:46:04 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:239)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:230)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@cresco6-nvi1.portici.enea.it:26833\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/05/08 11:46:04 WARN BlockManagerMasterEndpoint: Error trying to remove shuffle 10. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:26833 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from cresco6-nvi1.portici.enea.it:26833 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 11:46:08 WARN NettyRpcEnv: Ignored message: true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=====================================================>(498 + 2) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.1725690490868297\n",
      "R2 (Prediction Data): 0.9477559442679713\n",
      "MSE (Prediction Data): 1.3749181748763921\n",
      "MAE (Prediction Data): 0.3221226197209398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens = (\n",
    "    spark.read.parquet(\"output/decision_tree_regression_forecast_PM2_5/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi/output/random_forest_regression_forecast_PM2_5/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens = (\n",
    "    spark.read.parquet(\"output/gradient_boosted_regression_forecast_PM2_5/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions = (\n",
    "    predictions_dt_ens\n",
    "    .join(predictions_rf_ens, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .join(predictions_gbt_ens, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM2_5\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions.show(5)\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens = evaluator_ens.evaluate(combined_predictions, {evaluator_ens.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4612fb",
   "metadata": {},
   "source": [
    "Lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8af46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|           c_PM2_5|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6390908E9|          28.36781|462000.0|5016000.0|[1.6390908E9,4620...|29.509494765047442|\n",
      "|       1.6390944E9|24.782626999999998|462000.0|5016000.0|[1.6390944E9,4620...|25.770688211050427|\n",
      "|        1.639098E9|         22.501345|462000.0|5016000.0|[1.639098E9,46200...|22.860911274046998|\n",
      "|       1.6391016E9|         20.493872|462000.0|5016000.0|[1.6391016E9,4620...|20.618532512389326|\n",
      "|       1.6391052E9|         18.949131|462000.0|5016000.0|[1.6391052E9,4620...|18.636246233624217|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|           c_PM2_5|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|        0.80977404|482000.0|5088000.0|1.4367218441419283|\n",
      "|       1.6250904E9|         1.0493861|486000.0|5060000.0|1.9152067457223598|\n",
      "|       1.6250904E9|          2.703447|498000.0|5012000.0| 2.697859930249587|\n",
      "|       1.6250904E9|2.5132527000000002|510000.0|5028000.0|2.7900458910407493|\n",
      "|       1.6250904E9|          1.501193|518000.0|4972000.0|1.8873936130041187|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3756927823676735\n",
      "R2 (Prediction Data): 0.9849632477806485\n",
      "MSE (Prediction Data): 1.8925306314585117\n",
      "MAE (Prediction Data): 0.5007091074118767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM2_5_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM2_5_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM2_5_lombardia/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lombardia.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lombardia = (\n",
    "    predictions_dt_ens_lombardia\n",
    "    .join(predictions_rf_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .join(predictions_gbt_ens_lombardia, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM2_5\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lombardia.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lambordia = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lambordia = evaluator_ens_lambordia.evaluate(combined_predictions_lombardia, {evaluator_ens_lambordia.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lambordia}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lambordia}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lambordia}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lambordia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4d019",
   "metadata": {},
   "source": [
    "Lazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "505ae49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "|original_date_time|          c_PM2_5|       x|        y|            features|   prediction_gbt|\n",
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "|       1.6295832E9|        3.9943345|702000.0|4696000.0|[1.6295832E9,7020...|4.005598390432201|\n",
      "|       1.6295868E9|4.508026999999999|702000.0|4696000.0|[1.6295868E9,7020...|4.503454314178332|\n",
      "|       1.6295904E9|        4.8864765|702000.0|4696000.0|[1.6295904E9,7020...|4.806861088845714|\n",
      "|        1.629594E9|        5.1068788|702000.0|4696000.0|[1.629594E9,70200...|4.806861088845714|\n",
      "|       1.6295976E9|         5.176116|702000.0|4696000.0|[1.6295976E9,7020...|5.361793711605291|\n",
      "+------------------+-----------------+--------+---------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------+---------+------------------+\n",
      "|original_date_time|  c_PM2_5|       x|        y|        prediction|\n",
      "+------------------+---------+--------+---------+------------------+\n",
      "|       1.6250904E9|3.4351873|734000.0|4684000.0|3.3198713808305236|\n",
      "|       1.6250904E9|  2.70562|746000.0|4692000.0|2.7862845952834294|\n",
      "|       1.6250904E9|2.2483215|758000.0|4652000.0| 2.456650295078754|\n",
      "|       1.6250904E9| 2.263646|774000.0|4668000.0|2.4132768723066143|\n",
      "|       1.6250904E9|2.5161965|778000.0|4692000.0|2.6230998518079645|\n",
      "+------------------+---------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 357:===================================================>  (95 + 5) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 1.3497555798033924\n",
      "R2 (Prediction Data): 0.9396922092084667\n",
      "MSE (Prediction Data): 1.821840125210392\n",
      "MAE (Prediction Data): 0.3099513955177884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM2_5_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM2_5_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_lazio = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM2_5_lazio/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_lazio.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_lazio = (\n",
    "    predictions_dt_ens_lazio\n",
    "    .join(predictions_rf_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .join(predictions_gbt_ens_lazio, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM2_5\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_lazio.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_lazio = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_lazio = evaluator_ens_lazio.evaluate(combined_predictions_lazio, {evaluator_ens_lazio.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_lazio}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_lazio}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_lazio}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_lazio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d724a",
   "metadata": {},
   "source": [
    "Campania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7161ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|original_date_time|           c_PM2_5|       x|        y|            features|    prediction_gbt|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "|       1.6339032E9|          1.766012|902000.0|4576000.0|[1.6339032E9,9020...|1.7723182320484392|\n",
      "|       1.6339068E9|2.0191703000000003|902000.0|4576000.0|[1.6339068E9,9020...|2.0334144358319204|\n",
      "|       1.6339104E9|         2.3572073|902000.0|4576000.0|[1.6339104E9,9020...|2.2606373697290754|\n",
      "|        1.633914E9|2.6865732999999996|902000.0|4576000.0|[1.633914E9,90200...| 2.651789357333835|\n",
      "|       1.6339176E9|2.8155707999999997|902000.0|4576000.0|[1.6339176E9,9020...|2.8298444480703786|\n",
      "+------------------+------------------+--------+---------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+---------+------------------+\n",
      "|original_date_time|           c_PM2_5|       x|        y|        prediction|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "|       1.6250904E9|          2.039516|910000.0|4568000.0| 2.472822088430815|\n",
      "|       1.6250904E9|1.9091046999999999|910000.0|4576000.0|2.4665097583009317|\n",
      "|       1.6250904E9|1.8431568999999999|914000.0|4592000.0|2.3084423310166193|\n",
      "|       1.6250904E9|         2.4410367|934000.0|4572000.0| 2.540594725755868|\n",
      "|       1.6250904E9|         1.5820067|934000.0|4604000.0|2.2900944247825765|\n",
      "+------------------+------------------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 278:========================================>            (76 + 24) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Prediction Data): 2.1836734371962385\n",
      "R2 (Prediction Data): 0.8831054082102053\n",
      "MSE (Prediction Data): 4.7684296803164345\n",
      "MAE (Prediction Data): 0.3798199086679054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 278:====================================================> (97 + 3) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_dt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/decision_tree_regression_forecast_PM2_5_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_dt\")\n",
    ")\n",
    "predictions_rf_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/random_forest_regression_forecast_PM2_5_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    ")\n",
    "predictions_gbt_ens_campania = (\n",
    "    spark.read.parquet(\"/afs/enea.it/por/user/nafis/PFS/tmp/nafi2/nafi/output/gradient_boosted_regression_forecast_PM2_5_campania/\")\n",
    "    .withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    ")\n",
    "predictions_gbt_ens_campania.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Take the simple average of the predictions\n",
    "combined_predictions_campania = (\n",
    "    predictions_dt_ens_campania\n",
    "    .join(predictions_rf_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .join(predictions_gbt_ens_campania, [\"original_date_time\", \"x\", \"y\", \"c_PM2_5\"])\n",
    "    .withColumn(\"prediction\", (col(\"prediction_dt\") + col(\"prediction_rf\") + col(\"prediction_gbt\")) / 3.0)\n",
    "    .select(\"original_date_time\", \"c_PM2_5\", \"x\", \"y\", \"prediction\")\n",
    ")\n",
    "combined_predictions_campania.show(5)\n",
    "\n",
    "\n",
    "# Create a Regression Evaluator\n",
    "evaluator_ens_campania = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"c_PM2_5\")\n",
    "\n",
    "# Compute RMSE (root mean squared error)\n",
    "rmse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"rmse\"})\n",
    "\n",
    "# Compute R2 (coefficient of determination)\n",
    "r2_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"r2\"})\n",
    "\n",
    "# Compute MSE (mean squared error)\n",
    "mse_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mse\"})\n",
    "\n",
    "# Compute MAE (mean absolute error)\n",
    "mae_ens_campania = evaluator_ens_campania.evaluate(combined_predictions_campania, {evaluator_ens_campania.metricName: \"mae\"})\n",
    "\n",
    "# Print the results with metric names including \"Prediction Data\"\n",
    "print(f\"RMSE (Prediction Data): {rmse_ens_campania}\")\n",
    "print(f\"R2 (Prediction Data): {r2_ens_campania}\")\n",
    "print(f\"MSE (Prediction Data): {mse_ens_campania}\")\n",
    "print(f\"MAE (Prediction Data): {mae_ens_campania}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ae2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e448f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
